@article{emarketer.com_2016, title={US Digital Ad Spending to Surpass TV this Year}, journal={emarketer.com}, year={2016}, month={Sep}}


@online{buckle-2016,
  author = {Buckle, Chase},
  title = {Digital consumers own 3.64 connected devices},
  year = 2016,
  month = Feb,
  url = {
  https://www.globalwebindex.net/blog/digital-consumers-own-3.64-connected-devices},
  urldate = {2016-02-18}
}

    
@inproceedings{chan2010evaluating,
  title={Evaluating online ad campaigns in a pipeline: causal models at scale},
    author={Chan, David and Ge, Rong and Gershony, Ori and Hesterberg, Tim and Lambert, Diane},
      booktitle={Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining},
        pages={7--16},
          year={2010},
            organization={ACM}
            }

@inproceedings{coey2016people,
  title={People and cookies: Imperfect treatment assignment in online experiments},
    author={Coey, Dominic and Bailey, Michael},
      booktitle={Proceedings of the 25th International Conference on World Wide Web},
        pages={1103--1111},
          year={2016},
            organization={International World Wide Web Conferences Steering Committee}
            }

@incollection{elwert2013graphical,
  title={Graphical causal models},
    author={Elwert, Felix},
      booktitle={Handbook of causal analysis for social research},
        pages={245--273},
          year={2013},
            publisher={Springer}
            }

@article{gordon2016comparison,
  title={A comparison of approaches to advertising measurement: Evidence from big field experiments at Facebook},
    author={Gordon, Brett R and Zettelmeyer, Florian and Bhargava, Neha and Chapsky, Dan},
      journal={White paper},
        year={2016}
        }

@article{johansson2016learning,
  title={Learning representations for counterfactual inference},
    author={Johansson, Fredrik D and Shalit, Uri and Sontag, David},
      journal={arXiv preprint arXiv:1605.03661},
        year={2016}
        }

@techreport{resnik2010gibbs,
  title={Gibbs sampling for the uninitiated},
    author={Resnik, Philip and Hardisty, Eric},
      year={2010},
        institution={DTIC Document}
        }

@article{stitelman2011estimating,
  title={Estimating the effect of online display advertising on browser conversion},
    author={Stitelman, Ori and Dalessandro, Brian and Perlich, Claudia and Provost, Foster},
      journal={Data Mining and Audience Intelligence for Advertising (ADKDD 2011)},
        volume={8},
          year={2011}
          }
@article{athey2016recursive,
  title={Recursive partitioning for heterogeneous causal effects},
    author={Athey, Susan and Imbens, Guido},
      journal={Proceedings of the National Academy of Sciences},
        volume={113},
          number={27},
            pages={7353--7360},
              year={2016},
                publisher={National Acad Sciences}
                }

@article{Geyik_Multi_2015,
  year={2015},
  author={Geyik, Sahin and Saxena, Abhishek and Dasdan, Ali},
  title={{Multi-Touch} Attribution Based Budget Allocation in Online Advertising},
  abstract={Budget allocation in online advertising deals with distributing the campaign (insertion order) level budgets to different sub-campaigns which employ different targeting criteria and may perform differently in terms of return-on-investment {(ROI).} In this paper, we present the efforts at Turn on how to best allocate campaign budget so that the advertiser or campaign-level {ROI} is maximized. To do this, it is crucial to be able to correctly determine the performance of sub-campaigns. This determination is highly related to the action-attribution problem, i.e. to be able to find out the set of ads, and hence the sub-campaigns that provided them to a user, that an action should be attributed to. For this purpose, we employ both last-touch (last ad gets all credit) and multi-touch (many ads share the credit) attribution methodologies. We present the algorithms deployed at Turn for the attribution problem, as well as their parallel implementation on the large advertiser performance datasets. We conclude the paper with our empirical comparison of last-touch and multi-touch attribution-based budget allocation in a real online advertising setting.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Allocation/Turn_MultiTouchAttribBudgetAllocation.pdf}
}
@article{Tastle_Consensus_2007,
  year={2007},
  journal={Int J Approx Reason},
  author={Tastle, William and Wierman, Mark},
  volume={45},
  doi={10.1016/j.ijar.2006.06.024},
  title={Consensus and dissention: A measure of ordinal dispersion},
  number={3},
  issn={{0888-613X}},
  abstract={A new measure of dispersion is introduced as a representation of consensus (agreement) and dissention (disagreement). Building on the generally accepted Shannon entropy, this measure utilizes a probability distribution and the distance between categories to produce a value spanning the unit interval. The measure is applied to the Likert scale (or any ordinal scale) to determine degrees of consensus or agreement. Using this measure, data on ordinal scales can be given a value of dispersion that is both logically and theoretically sound.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Ari/OrdinalDispersion_tastle2007.pdf},
  pages={531-545}
}
@article{Singh_Dynamic_2015,
  year={2015},
  author={Singh, Vivek and Dutta, Kaushik},
  doi={10.1109/HICSS.2015.184},
  title={Dynamic Price Prediction for Amazon Spot Instances},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/AWS/DynamicAWSSpotPrediction.pdf},
  pages={1513-1520}
}
@article{Agarwal_A_2016,
  year={2016},
  author={Agarwal, Alekh and Bird, Sarah and Cozowicz, Markus and Hoang, Luong and Langford, John and Lee, Stephen and Li, Jiaji and Melamed, Dan and Oshri, Gal and Ribas, Oswaldo and Sen, Siddhartha and Slivkins, Alex},
  title={A Multiworld Testing Decision Service},
  abstract={Applications and systems are constantly faced with decisions to make, often using a policy to pick from a set of actions based on some contextual information. We create a service that uses machine learning to accomplish this goal. The service uses exploration, logging, and online learning to create a counterfactually sound system supporting a full data lifecycle. The system is general: it works for any discrete choices, with respect to any reward metric, and can work with many learning algorithms and feature representations. The service has a simple {API,} and was designed to be modular and reproducible to ease deployment and debugging, respectively. We demonstrate how these properties enable learning systems that are robust and safe. Our evaluation shows that the Decision Service makes decisions in real time and incorporates new data quickly into learned policies. A large-scale deployment for a personalized news website has been handling all traffic since Jan. 2016, resulting in a 25\% relative lift in clicks. By making the Decision Service externally available, we hope to make optimal decision making available to all.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Bandit/MSFT_MWT-WhitePaper.pdf}
}
@article{Cui_Bid_2011,
  year={2011},
  author={Cui, Ying and Zhang, Ruofei and Li, Wei and Mao, Jianchang},
  doi={10.1145/2020408.2020454},
  title={Bid landscape forecasting in online ad exchange marketplace},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Bidding/Cui_BidLandscapeForecastingDispla.pdf},
  pages={265}
}
@book{Ghosh_Adaptive_2009,
  year={2009},
  abstract={Motivated by the emergence of auction-based marketplaces for display ads such as the Right Media Exchange, we study the design of a bidding agent that implements a display advertising campaign by bidding in such a marketplace. The bidding agent must acquire a given number of impressions with a given target spend, when the highest external bid in the marketplace is drawn from an unknown distribution P. The quantity and spend constraints arise from the fact that display ads are usually sold on a {CPM} basis. We consider both the full information setting, where the winning price in each auction is announced publicly, and the partially observable setting where only the winner obtains information about the distribution; these differ in the penalty incurred by the agent while attempting to learn the distribution. We provide algorithms for both settings, and prove performance guarantees using bounds on uniform closeness from statistics, and techniques from online learning. We experimentally evaluate these algorithms: both algorithms perform very well with respect to both target quantity and spend; further, our algorithm for the partially observable case performs nearly as well as that for the fully observable setting despite the higher penalty incurred during learning.},
  doi={10.1145/1526709.1526744},
  isbn={9781605584874},
  author={Ghosh, Arpita and Rubinstein, Benjamin and Vassilvitskii, Sergei and Zinkevich, Martin},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Bidding/Ghosh_AdaptiveBiddingDisplayAds.pdf;/Users/pchalasani/Dropbox/MediaMath/Papers/Mendeley/Vassilvitskii, Rubinstein, Zinkevich - 2009 - Adaptive Bidding for Display Advertising Categories and Subject Descriptors.pdf}
}
@book{Zhang_Optimal,
  year={0},
  abstract={In this paper we study bid optimisation for real-time bidding {(RTB)} based display advertising. {RTB} allows advertisers to bid on a display ad impression in real time when it is being generated. It goes beyond contextual advertising by motivating the bidding focused on user data and it is different from the sponsored search auction where the bid price is associated with keywords. For the demand side, a fundamental technical challenge is to automate the bidding process based on the budget, the campaign objective and various information gathered in runtime and in history. In this paper, the programmatic bidding is cast as a functional optimisation problem. Under certain dependency assumptions, we derive simple bidding functions that can be calculated in real time; our finding shows that the optimal bid has a non-linear relationship with the impression level evaluation such as the click-through rate and the conversion rate, which are estimated in real time from the impression level features. This is different from previous work that is mainly focused on a linear bidding function. Our mathematical derivation suggests that optimal bidding strategies should try to bid more impressions rather than focus on a small set of high valued impressions because according to the current {RTB} market data, compared to the higher evaluated impressions, the lower evaluated ones are more cost effective and the chances of winning them are relatively higher. Aside from the theoretical insights, offline experiments on a real dataset and online experiments on a production {RTB} system verify the effectiveness of our proposed optimal bidding strategies and the functional optimisation framework.},
  doi={10.1145/2623330.2623633},
  isbn={9781450329569},
  author={Zhang, Weinan and Yuan, Shuai and Wang, Jun},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Bidding/KDD-2014-optimal-real-time-bidding.pdf;/Users/pchalasani/Dropbox/MediaMath/Papers/Bidding/Slides_RTB.pdf;/Users/pchalasani/Dropbox/MediaMath/Papers/Bidding/Zhang_OptimalRealTimeBidding.pdf}
}
@article{Perlich_Bid_2012,
  year={2012},
  author={Perlich, Claudia and Dalessandro, Brian and Hook, Rod and Stitelman, Ori and Raeder, Troy and Provost, Foster},
  doi={10.1145/2339530.2339655},
  title={Bid optimizing and inventory scoring in targeted online advertising},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Bidding/Perlich_BidOptimizationInventoryScoring.pdf},
  pages={804}
}
@book{Adikari_New_2015,
  year={2015},
  abstract={Real time bidding {(RTB)} is becoming the key to target marketing where it could optimize advertiser expectations drastically. Not like the conventional digital advertising, in the process of {RTB,} the impressions of a mobile application or a website are mapped to a particular advertiser through a bidding process which triggers and held for a few milliseconds after an application is launched. To carry out the bidding process a special platform called demand side platform {(DSP)} provides support to advertisers to bid for available impressions on their behalf. This process has turned into a complex mission as there are many applications/websites that have come into the market. Mapping them to advertisers’ target audience, and bidding appropriately for them is not a simple human mediated process. The complexity and the dynamic nature in the {RTB} process make it difficult to apply forecasting strategies effectively and efficiently. In this paper we propose an autonomous and a dynamic strategy for bidding decisions such as bidding price. We applied our proposed approach on a real {RTB} bidding data and demonstrated that our approach can achieve higher conversion rate with the target spend for a {DSP.}},
  volume={9073},
  doi={10.1007/978-3-319-18714-3_2},
  title={Real Time Bidding in Online Digital Advertisement},
  isbn={9783319187136},
  author={Adikari, Shalinda and Dutta, Kaushik},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Bidding/RealTimeBidding.pdf}
}
@article{Lee_Real_2013,
  year={2013},
  author={Lee, {Kuang-Chih} and Jalali, Ali and Dasdan, Ali},
  title={Real Time Bid Optimization with Smooth Budget Delivery in Online Advertising},
  abstract={Today, billions of display ad impressions are purchased on a daily basis through a public auction hosted by real time bidding {(RTB)} exchanges. A decision has to be made for advertisers to submit a bid for each selected {RTB} ad request in milliseconds. Restricted by the budget, the goal is to buy a set of ad impressions to reach as many targeted users as possible. A desired action (conversion), advertiser specific, includes purchasing a product, filling out a form, signing up for emails, etc. In addition, advertisers typically prefer to spend their budget smoothly over the time in order to reach a wider range of audience accessible throughout a day and have a sustainable impact. However, since the conversions occur rarely and the occurrence feedback is normally delayed, it is very challenging to achieve both budget and performance goals at the same time. In this paper, we present an online approach to the smooth budget delivery while optimizing for the conversion performance. Our algorithm tries to select high quality impressions and adjust the bid price based on the prior performance distribution in an adaptive manner by distributing the budget optimally across time. Our experimental results from real advertising campaigns demonstrate the effectiveness of our proposed approach.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Bidding/Turn_RealTimeBidOptimizationSmoothBudget.pdf}
}
@article{Yuan_Real_2013,
  year={2013},
  author={Yuan, Shuai and Wang, Jun and Zhao, Xiaoxue},
  title={Real-time Bidding for Online Advertising: Measurement and Analysis},
  abstract={The real-time bidding {(RTB),} aka programmatic buying, has recently become the fastest growing area in online advertising. Instead of bulking buying and inventory-centric buying, {RTB} mimics stock exchanges and utilises computer algorithms to automatically buy and sell ads in real-time; It uses per impression context and targets the ads to specific people based on data about them, and hence dramatically increases the effectiveness of display advertising. In this paper, we provide an empirical analysis and measurement of a production ad exchange. Using the data sampled from both demand and supply side, we aim to provide first-hand insights into the emerging new impression selling infrastructure and its bidding behaviours, and help identifying research and design issues in such systems. From our study, we observed that periodic patterns occur in various statistics including impressions, clicks, bids, and conversion rates (both post-view and post-click), which suggest time-dependent models would be appropriate for capturing the repeated patterns in {RTB.} We also found that despite the claimed second price auction, the first price payment in fact is accounted for 55.4\% of total cost due to the arrangement of the soft floor price. As such, we argue that the setting of soft floor price in the current {RTB} systems puts advertisers in a less favourable position. Furthermore, our analysis on the conversation rates shows that the current bidding strategy is far less optimal, indicating the significant needs for optimisation algorithms incorporating the facts such as the temporal behaviours, the frequency and recency of the ad displays, which have not been well considered in the past.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Bidding/Wang_RTBOnlineAdvertising.pdf;/Users/pchalasani/Dropbox/MediaMath/Papers/Bidding/Yuan_RTBMeasurementAnalysis.pdf}
}
@book{Wu_Predicting_2015,
  year={2015},
  abstract={In the aspect of a {Demand-Side} Platform {(DSP),} which is the agent of advertisers, we study how to predict the winning price such that the {DSP} can win the bid by placing a proper bidding value in the real-time bidding {(RTB)} auction. We propose to leverage the machine learning and statistical methods to train the winning price model from the bidding history. A major challenge is that a {DSP} usually suffers from the censoring of the winning price, especially for those lost bids in the past. To solve it, we utilize the censored regression model, which is widely used in the survival analysis and econometrics, to fit the censored bidding data. Note, however, the assumption of censored regression does not hold on the real {RTB} data. As a result, we further propose a mixture model, which combines linear regression on bids with observable winning prices and censored regression on bids with the censored winning prices, weighted by the winning rate of the {DSP.} Experiment results show that the proposed mixture model in general prominently outperforms linear regression in terms of the prediction accuracy.},
  doi={10.1145/2783258.2783276},
  isbn={9781450336642},
  author={Wu, Wush and Yeh, {Mi-Yen} and Chen, {Ming-Syan}},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Bidding/Wush_PredictingWinningPriceCensored.pdf}
}
@book{Zhang_Feedback_2016,
  year={2016},
  abstract={{Real-Time} Bidding {(RTB)} is revolutionising display advertising by facilitating per-impression auctions to buy ad impressions as they are being generated. Being able to use impression-level data, such as user cookies, encourages user behaviour targeting, and hence has significantly improved the effectiveness of ad campaigns. However, a fundamental drawback of {RTB} is its instability because the bid decision is made per impression and there are enormous fluctuations in campaigns' key performance indicators {(KPIs).} As such, advertisers face great difficulty in controlling their campaign performance against the associated costs. In this paper, we propose a feedback control mechanism for {RTB} which helps advertisers dynamically adjust the bids to effectively control the {KPIs,} e.g., the auction winning ratio and the effective cost per click. We further formulate an optimisation framework to show that the proposed feedback control mechanism also has the ability of optimising campaign performance. By settling the effective cost per click at an optimal reference value, the number of campaign's ad clicks can be maximised with the budget constraint. Our empirical study based on real-world data verifies the effectiveness and robustness of our {RTB} control system in various situations. The proposed feedback control mechanism has also been deployed on a commercial {RTB} platform and the online test has shown its success in generating controllable advertising performance.},
  doi={10.1145/2835776.2835843},
  isbn={9781450337168},
  author={Zhang, Weinan and Rong, Yifei and Wang, Jun and Zhu, Tianchi and Wang, Xiaofan},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Bidding/Zhang2016_FeedbackControlRTB.pdf}
}
@book{Zhang_Bid,
  year={0},
  abstract={In real-time display advertising, ad slots are sold per impression via an auction mechanism. For an advertiser, the campaign information is incomplete --- the user responses (e.g, clicks or conversions) and the market price of each ad impression are observed only if the advertiser's bid had won the corresponding ad auction. The predictions, such as bid landscape forecasting, click-through rate {(CTR)} estimation, and bid optimisation, are all operated in the pre-bid stage with full-volume bid request data. However, the training data is gathered in the post-bid stage with a strong bias towards the winning impressions. A common solution for learning over such censored data is to reweight data instances to correct the discrepancy between training and prediction. However, little study has been done on how to obtain the weights independent of previous bidding strategies and consequently integrate them into the final {CTR} prediction and bid generation steps. In this paper, we formulate {CTR} estimation and bid optimisation under such censored auction data. Derived from a survival model, we show that historic bid information is naturally incorporated to produce Bid-aware Gradient Descents {(BGD)} which controls both the importance and the direction of the gradient to achieve unbiased learning. The empirical study based on two large-scale real-world datasets demonstrates remarkable performance gains from our solution. The learning framework has been deployed on Yahoo!'s real-time bidding platform and provided 2.97\% {AUC} lift for {CTR} estimation and 9.30\% {eCPC} drop for bid optimisation in an online {A/B} test.},
  doi={10.1145/2939672.2939713},
  isbn={9781450342322},
  author={Zhang, Weinan and Zhou, Tianxiong and Wang, Jun and Xu, Jian},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Bidding/Zhang_BidAwareGradientDescent.pdf}
}
@article{Zhang_Deep_2016,
  year={2016},
  author={Zhang, Weinan and Du, Tianming and Wang, Jun},
  title={Deep Learning over Multi-field Categorical Data: A Case Study on User Response Prediction},
  abstract={Predicting user responses, such as click-through rate and conversion rate, are critical in many web applications including web search, personalised recommendation, and online advertising. Different from continuous raw features that we usually found in the image and audio domains, the input features in web space are always of multi-field and are mostly discrete and categorical while their dependencies are little known. Major user response prediction models have to either limit themselves to linear models or require manually building up high-order combination features. The former loses the ability of exploring feature interactions, while the latter results in a heavy computation in the large feature space. To tackle the issue, we propose two novel models using deep neural networks {(DNNs)} to automatically learn effective patterns from categorical feature interactions and make predictions of users' ad clicks. To get our {DNNs} efficiently work, we propose to leverage three feature transformation methods, i.e., factorisation machines {(FMs),} restricted Boltzmann machines {(RBMs)} and denoising auto-encoders {(DAEs).} This paper presents the structure of our models and their efficient training algorithms. The large-scale experiments with real-world data demonstrate that our methods work better than major state-of-the-art models.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Bidding/Zhang_DeepLearningCategoricalResponsePrediction.pdf}
}
@article{Zhang_Real_2014,
  year={2014},
  author={Zhang, Weinan and Yuan, Shuai and Wang, Jun and Shen, Xuehua},
  title={{Real-Time} Bidding Benchmarking with {iPinYou} Dataset},
  abstract={Being an emerging paradigm for display advertising, {Real-Time} Bidding {(RTB)} drives the focus of the bidding strategy from context to users' interest by computing a bid for each impression in real time. The data mining work and particularly the bidding strategy development becomes crucial in this performance-driven business. However, researchers in computational advertising area have been suffering from lack of publicly available benchmark datasets, which are essential to compare different algorithms and systems. Fortunately, a leading Chinese advertising technology company {iPinYou} decided to release the dataset used in its global {RTB} algorithm competition in 2013. The dataset includes logs of ad auctions, bids, impressions, clicks, and final conversions. These logs reflect the market environment as well as form a complete path of users' responses from advertisers' perspective. This dataset directly supports the experiments of some important research problems such as bid optimisation and {CTR} estimation. To the best of our knowledge, this is the first publicly available dataset on {RTB} display advertising. Thus, they are valuable for reproducible research and understanding the whole {RTB} ecosystem. In this paper, we first provide the detailed statistical analysis of this dataset. Then we introduce the research problem of bid optimisation in {RTB} and the simple yet comprehensive evaluation protocol. Besides, a series of benchmark experiments are also conducted, including both click-through rate {(CTR)} estimation and bid optimisation.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Bidding/Zhang_RTBBenchmarking.pdf}
}
@article{Singh_Ensemble_2010,
  year={2010},
  journal={Mach Learn},
  author={Singh, Vikas and Mukherjee, Lopamudra and Peng, Jiming and Xu, Jinhui},
  volume={79},
  doi={10.1007/s10994-009-5158-y},
  title={Ensemble clustering using semidefinite programming with applications},
  number={1-2},
  pmid={21927539},
  issn={0885-6125},
  abstract={In this paper, we study the ensemble clustering problem, where the input is in the form of multiple clustering solutions. The goal of ensemble clustering algorithms is to aggregate the solutions into one solution that maximizes the agreement in the input ensemble. We obtain several new results for this problem. Specifically, we show that the notion of agreement under such circumstances can be better captured using a {2D} string encoding rather than a voting strategy, which is common among existing approaches. Our optimization proceeds by first constructing a non-linear objective function which is then transformed into a 0-1 Semidefinite program {(SDP)} using novel convexification techniques. This model can be subsequently relaxed to a polynomial time solvable {SDP.} In addition to the theoretical contributions, our experimental results on standard machine learning and synthetic datasets show that this approach leads to improvements not only in terms of the proposed agreement measure but also the existing agreement measures based on voting strategies. In addition, we identify several new application scenarios for this problem. These include combining multiple image segmentations and generating tissue maps from multiple-channel Diffusion Tensor brain images to identify the underlying structure of the brain.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/CID/Bailey_InformationTheoreticClusterComparison.pdf},
  pages={177-200}
}
@article{Diaz-Morales_Cross_2015,
  year={2015},
  author={{Diaz-Morales,} Roberto},
  doi={10.1109/ICDMW.2015.244},
  title={{Cross-Device} Tracking: Matching Devices and Cookies},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/CID/diaz-morales2015_MatchingDevicesCookies.pdf},
  pages={1699-1704}
}
@article{Goldberg_Measuring_2010,
  year={2010},
  author={Goldberg, Mark and Hayvanovych, Mykola and {Magdon-Ismail,} Malik},
  doi={10.1109/SocialCom.2010.50},
  title={Measuring Similarity between Sets of Overlapping Clusters},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/CID/Goldberg_SimilarityOverlappingClusters.pdf},
  pages={303-308}
}
@article{Meil_Comparing_2007,
  year={2007},
  journal={J Multivariate Anal},
  author={Meilă, Marina},
  volume={98},
  doi={10.1016/j.jmva.2006.11.013},
  title={Comparing clusterings—an information based distance},
  number={5},
  issn={{0047-259X}},
  abstract={This paper proposes an information theoretic criterion for comparing two partitions, or clusterings, of the same data set. The criterion, called variation of information {(VI),} measures the amount of information lost and gained in changing from clustering C to clustering C′. The basic properties of {VI} are presented and discussed. We focus on two kinds of properties: (1) those that help one build intuition about the new criterion (in particular, it is shown the {VI} is a true metric on the space of clusterings), and (2) those that pertain to the comparability of {VI} values over different experimental conditions. As the latter properties have rarely been discussed explicitly before, other existing comparison criteria are also examined in their light. Finally we present the {VI} from an axiomatic point of view, showing that it is the only “sensible” criterion for comparing partitions that is both aligned to the lattice and convexely additive. As a consequence, we prove an impossibility result for comparing partitions: there is no criterion for comparing partitions that simultaneously satisfies the above two desirable properties and is bounded.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/CID/Melia_ComparingClusterings_FULL.pdf},
  pages={873-895}
}
@article{Sokolova_A_2009,
  year={2009},
  journal={Inform Process Manag},
  author={Sokolova, Marina and Lapalme, Guy},
  volume={45},
  doi={10.1016/j.ipm.2009.03.002},
  title={A systematic analysis of performance measures for classification tasks},
  number={4},
  issn={0306-4573},
  abstract={This paper presents a systematic analysis of twenty four performance measures used in the complete spectrum of Machine Learning classification tasks, i.e., binary, multi-class, multi-labelled, and hierarchical. For each classification task, the study relates a set of changes in a confusion matrix to specific characteristics of data. Then the analysis concentrates on the type of changes to a confusion matrix that do not change a measure, therefore, preserve a classifier’s evaluation (measure invariance). The result is the measure invariance taxonomy with respect to all relevant label distribution changes in a classification problem. This formal analysis is supported by examples of applications where invariance properties of measures lead to a more reliable evaluation of classifiers. Text classification supplements the discussion with several case studies.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/CID/SokolovaLapalme-SystematicStudyClassificationMetrics.pdf},
  pages={427-437}
}
@article{Signorini_Kernel_2004,
  year={2004},
  journal={J Am Stat Assoc},
  author={Signorini, D and Jones, M},
  volume={99},
  doi={10.1198/016214504000000115},
  title={Kernel Estimators for Univariate Binary Regression},
  number={465},
  issn={0162-1459},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Cmv/signorini_univariateKernelRegression.pdf},
  pages={119-126}
}
@article{Jalali_Scalable_2013,
  year={2013},
  author={Jalali, Ali and Kolay, Santanu and Foldes, Peter and Dasdan, Ali},
  title={Scalable Audience Reach Estimation in Real-time Online Advertising},
  abstract={Online advertising has been introduced as one of the most efficient methods of advertising throughout the recent years. Yet, advertisers are concerned about the efficiency of their online advertising campaigns and consequently, would like to restrict their ad impressions to certain websites and/or certain groups of audience. These restrictions, known as targeting criteria, limit the reachability for better performance. This trade-off between reachability and performance illustrates a need for a forecasting system that can quickly predict/estimate (with good accuracy) this trade-off. Designing such a system is challenging due to (a) the huge amount of data to process, and, (b) the need for fast and accurate estimates. In this paper, we propose a distributed fault tolerant system that can generate such estimates fast with good accuracy. The main idea is to keep a small representative sample in memory across multiple machines and formulate the forecasting problem as queries against the sample. The key challenge is to find the best strata across the past data, perform multivariate stratified sampling while ensuring fuzzy fall-back to cover the small minorities. Our results show a significant improvement over the uniform and simple stratified sampling strategies which are currently widely used in the industry.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Forecast/Arxiv_ScalableAudienceReachEstimation.pdf;/Users/pchalasani/Dropbox/MediaMath/Papers/Forecast/Turn_AudienceReachEstimation.pdf}
}
@article{Shrivastava_Asymmetric_2014,
  year={2014},
  author={Shrivastava, Anshumali and Li, Ping},
  title={Asymmetric Minwise Hashing},
  abstract={Minwise hashing {(Minhash)} is a widely popular indexing scheme in practice. Minhash is designed for estimating set resemblance and is known to be suboptimal in many applications where the desired measure is set overlap (i.e., inner product between binary vectors) or set containment. Minhash has inherent bias towards smaller sets, which adversely affects its performance in applications where such a penalization is not desirable. In this paper, we propose asymmetric minwise hashing {(MH-ALSH),} to provide a solution to this problem. The new scheme utilizes asymmetric transformations to cancel the bias of traditional minhash towards smaller sets, making the final "collision probability" monotonic in the inner product. Our theoretical comparisons show that for the task of retrieving with binary inner products asymmetric minhash is provably better than traditional minhash and other recently proposed hashing algorithms for general inner products. Thus, we obtain an algorithmic improvement over existing approaches in the literature. Experimental evaluations on four publicly available high-dimensional datasets validate our claims and the proposed scheme outperforms, often significantly, other hashing algorithms on the task of near neighbor retrieval with set containment. Our proposal is simple and easy to implement in practice.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Forecast/AsymmetricMinHash.pdf}
}
@book{Yu_Crossroads_2014,
  year={2014},
  abstract={The explosive increase in cellular network traffic, users, and applications, as well as the corresponding shifts in user expectations, has created heavy needs and demands on cellular data providers. In this paper we address one such need: mining the logs of cellular voice and data traffic to rapidly detect network performance anomalies and other events of interest. The core challenge in solving this problem is the issue that it is impossible to predict beforehand where in the traffic the event may appear, requiring us to be able to query arbitrary subsets of the network traffic (e.g., longer than usual round-trip times for users in a specific urban area to connect to {FunContent.com} using a particular model of phone). Since it is infeasible to store all combinations of such data, especially when it is collected in real-time, we need to be able to summarize the traffic data using succinct sketch data structures to answer these queries. The major contribution of this paper is the introduction of a scheme, called Crossroads, that can be used to compute the intersection of the measurements between two overlapping streams. For instance, in the above example, it is possible to compute the intersection of all the data going between the downtown area and {FunContent.com} with all the data generated by the model of phone to detect anomalous {RTT} behavior. In effect, this gives us a way to essentially "square root" the number of sketches that we need to maintain, transforming a prohibitively expensive problem to one that is tractable in practice. We provide rigorous analysis of our sketch and the trade-offs between memory footprint and accuracy. We also demonstrate the efficacy of our solution via simulation on data collected at a major cellular service carrier in the {US.}},
  doi={10.1145/2663716.2663733},
  isbn={9781450332132},
  author={Yu, Zhenglin and Ge, Zihui and Lall, Ashwin and Wang, Jia and Xu, Jun and Yan, He},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Forecast/ATT_Crossroads_PracticalSketchMiningIntersections.pdf}
}
@article{Aouiche_A_2007,
  year={2007},
  author={Aouiche, Kamel and Lemire, Daniel},
  title={A Comparison of Five Probabilistic {View-Size} Estimation Techniques in {OLAP}},
  abstract={A data warehouse cannot materialize all possible views, hence we must estimate quickly, accurately, and reliably the size of views to determine the best candidates for materialization. Many available techniques for view-size estimation make particular statistical assumptions and their error can be large. Comparatively, unassuming probabilistic techniques are slower, but they estimate accurately and reliability very large view sizes using little memory. We compare five unassuming hashing-based view-size estimation techniques including Stochastic Probabilistic Counting and {LogLog} Probabilistic Counting. Our experiments show that only Generalized Counting, {Gibbons-Tirthapura,} and Adaptive Counting provide universally tight estimates irrespective of the size of the view; of those, only Adaptive Counting remains constantly fast as we increase the memory budget.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Forecast/Comparison5UnassumingViewSizeEstimation.pdf}
}
@article{Cormode_An_2005,
  year={2005},
  journal={J Algorithm},
  author={Cormode, Graham and Muthukrishnan, S},
  volume={55},
  doi={10.1016/j.jalgor.2003.12.001},
  title={An improved data stream summary: the count-min sketch and its applications},
  number={1},
  issn={0196-6774},
  abstract={We introduce a new sublinear space data structure—the count-min sketch—for summarizing data streams. Our sketch allows fundamental queries in data stream summarization such as point, range, and inner product queries to be approximately answered very quickly; in addition, it can be applied to solve several important problems in data streams such as finding quantiles, frequent items, etc. The time and space bounds we show for using the {CM} sketch to solve these problems significantly improve those previously known—typically from 1/ε2 to 1/ε in factor.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Forecast/Cormode_SketchingStreamsNet.pdf},
  pages={58-75}
}
@article{Dasgupta_A_2015,
  year={2015},
  author={Dasgupta, Anirban and Lang, Kevin and Rhodes, Lee and Thaler, Justin},
  title={A Framework for Estimating Stream Expression Cardinalities},
  abstract={Given \$m\$ distributed data streams {\$A\_1,} \{\textbackslash\}dots, A\_m\$, we consider the problem of estimating the number of unique identifiers in streams defined by set expressions over {\$A\_1,} \{\textbackslash\}dots, A\_m\$. We identify a broad class of algorithms for solving this problem, and show that the estimators output by any algorithm in this class are perfectly unbiased and satisfy strong variance bounds. Our analysis unifies and generalizes a variety of earlier results in the literature. To demonstrate its generality, we describe several novel sampling algorithms in our class, and show that they achieve a novel tradeoff between accuracy, space usage, update speed, and applicability.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Forecast/Dasgupta_CardinalityEstimation.pdf}
}
@article{Zhu_LSH_2016,
  year={2016},
  author={Zhu, Erkang and Nargesian, Fatemeh and Pu, Ken and Miller, Ren{\'e}e},
  title={{LSH} Ensemble: {Internet-Scale} Domain Search},
  abstract={We study the problem of domain search where a domain is a set of distinct values from an unspecified universe. We use Jaccard set containment, defined as {\$\{\textbar\}Q} \{\textbackslash\}cap {X\{\textbar\}/\{\textbar\}Q\{\textbar\}\$,} as the relevance measure of a domain {\$X\$} to a query domain {\$Q\$.} Our choice of Jaccard set containment over Jaccard similarity makes our work particularly suitable for searching Open Data and data on the web, as Jaccard similarity is known to have poor performance over sets with large differences in their domain sizes. We demonstrate that the domains found in several real-life Open Data and web data repositories show a power-law distribution over their domain sizes. We present a new index structure, Locality Sensitive Hashing {(LSH)} Ensemble, that solves the domain search problem using set containment at Internet scale. Our index structure and search algorithm cope with the data volume and skew by means of data sketches {(MinHash)} and domain partitioning. Our index structure does not assume a prescribed set of values. We construct a cost model that describes the accuracy of {LSH} Ensemble with any given partitioning. This allows us to formulate the partitioning for {LSH} Ensemble as an optimization problem. We prove that there exists an optimal partitioning for any distribution. Furthermore, for datasets following a power-law distribution, as observed in Open Data and Web data corpora, we show that the optimal partitioning can be approximated using equi-depth, making it efficient to use in practice. We evaluate our algorithm using real data {(Canadian} Open Data and {WDC} Web Tables) containing up over 262 M domains. The experiments demonstrate that our index consistently outperforms other leading alternatives in accuracy and performance. The improvements are most dramatic for data with large skew in the domain sizes. Even at 262 M domains, our index sustains query performance with under 3 seconds response time.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Forecast/DomainSearchAtScale.pdf;/Users/pchalasani/Dropbox/MediaMath/Papers/Forecast/LSH_Ensemble_DomainSearchScale.pdf}
}
@article{Tzoumas_Sharing_2010,
  year={2010},
  journal={Proc Vldb Endow},
  author={Tzoumas, Kostas and Deshpande, Amol and Jensen, Christian},
  volume={3},
  doi={10.14778/1920841.1920911},
  title={Sharing-aware horizontal partitioning for exploiting correlations during query processing},
  number={1-2},
  issn={2150-8097},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Forecast/Jensen_LightweightGraphicalModelsQuery.pdf},
  pages={542-553}
}
@article{Jalali_On_2011,
  year={2011},
  author={Jalali, Ali and Johnson, Chris and Ravikumar, Pradeep},
  title={On Learning Discrete Graphical Models Using Greedy Methods},
  abstract={In this paper, we address the problem of learning the structure of a pairwise graphical model from samples in a high-dimensional setting. Our first main result studies the sparsistency, or consistency in sparsity pattern recovery, properties of a forward-backward greedy algorithm as applied to general statistical models. As a special case, we then apply this algorithm to learn the structure of a discrete graphical model via neighborhood estimation. As a corollary of our general result, we derive sufficient conditions on the number of samples n, the maximum node-degree d and the problem size p, as well as other conditions on the model parameters, so that the algorithm recovers all the edges with high probability. Our result guarantees graph selection for samples scaling as n = Omega(d\{\textasciicircum\}2 log(p)), in contrast to existing convex-optimization based algorithms that require a sample complexity of {\{\textbackslash\}Omega(d\{\textasciicircum\}3} log(p)). Further, the greedy algorithm only requires a restricted strong convexity condition which is typically milder than irrepresentability assumptions. We corroborate these results using numerical simulations at the end.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Forecast/Johnson_learning-discrete-graphical-models-using-greedy-methods.pdf}
}
@book{Agarwal_Knowing,
  year={0},
  abstract={Modern data analytics applications typically process massive amounts of data on clusters of tens, hundreds, or thousands of machines to support near-real-time {decisions.The} quantity of data and limitations of disk and memory bandwidth often make it infeasible to deliver answers at interactive speeds. However, it has been widely observed that many applications can tolerate some degree of inaccuracy. This is especially true for exploratory queries on data, where users are satisfied with "close-enough" answers if they can come quickly. A popular technique for speeding up queries at the cost of accuracy is to execute each query on a sample of data, rather than the whole dataset. To ensure that the returned result is not too inaccurate, past work on approximate query processing has used statistical techniques to estimate "error bars" on returned results. However, existing work in the sampling-based approximate query processing {(S-AQP)} community has not validated whether these techniques actually generate accurate error bars for real query workloads. In fact, we find that error bar estimation often fails on real world production workloads. Fortunately, it is possible to quickly and accurately diagnose the failure of error estimation for a query. In this paper, we show that it is possible to implement a query approximation pipeline that produces approximate answers and reliable error bars at interactive speeds.},
  doi={10.1145/2588555.2593667},
  isbn={9781450323765},
  author={Agarwal, Sameer and Milner, Henry and Kleiner, Ariel and Talwalkar, Ameet and Jordan, Michael and Madden, Samuel and Mozafari, Barzan and Stoica, Ion},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Forecast/JordanAgarwal_KnowWhenWrongApproximateQueryProcessing.pdf}
}
@book{Mehrotra_Selectivity_2001,
  year={2001},
  abstract={Estimating the result size of complex queries that involve selection on multiple attributes and the join of several relations is a difficult but fundamental task in database query processing. It arises in cost-based query optimization, query profiling, and approximate query answering. In this paper, we show how probabilistic graphical models can be effectively used for this task as an accurate and compact approximation of the joint frequency distribution of multiple attributes across multiple relations. Probabilistic Relational Models {(PRMs)} are a recent development that extends graphical statistical models such as Bayesian Networks to relational domains. They represent the statistical dependencies between attributes within a table, and between attributes across foreign-key joins. We provide an efficient algorithm for constructing a {PRM} front a database, and show how a {PRM} can be used to compute selectivity estimates for a broad class of queries. One of the major contributions of this work is a unified framework for the estimation of queries involving both select and foreign-key join operations. Furthermore, our approach is not limited to answering a small set of predetermined queries; a single model can be used to effectively estimate the sizes of a wide collection of potential queries across multiple tables. We present results for our approach on several real-world databases. For both single-table multi-attribute queries and a general class of select-join queries, our approach produces more accurate estimates than standard approaches to selectivity estimation, using comparable space and time.},
  volume={30},
  doi={10.1145/376284.375727},
  isbn={9781581133324},
  issn={0163-5808},
  author={Mehrotra, Sharad and Getoor, Lise and Taskar, Benjamin and Koller, Daphne},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Forecast/Kholler_SelectivityEstimation.pdf}
}
@article{Lowd_Naive_2005,
  year={2005},
  author={Lowd, Daniel and Domingos, Pedro},
  doi={10.1145/1102351.1102418},
  title={Naive Bayes models for probability estimation},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Forecast/Lowd_NaiveBayesProbabilityEstimation.pdf},
  pages={529-536}
}
@book{Pagh_Is,
  year={0},
  abstract={Min-wise hashing is an important method for estimating the size of the intersection of sets, based on a succinct summary (a "min-hash") of each set. One application is estimation of the number of data points that satisfy the conjunction of m \{\textgreater\}= 2 simple predicates, where a min-hash is available for the set of points satisfying each predicate. This has application in query optimization and for approximate computation of {COUNT} aggregates. In this paper we address the question: How many bits is it necessary to allocate to each summary in order to get an estimate with (1 +/- epsilon)-relative error? The state-of-the-art technique for minimizing the encoding size, for any desired estimation error, is b-bit min-wise hashing due to Li and K{\"o}nig {(Communications} of the {ACM,} 2011). We give new lower and upper bounds: Using information complexity arguments, we show that b-bit min-wise hashing is em space optimal for m=2 predicates in the sense that the estimator's variance is within a constant factor of the smallest possible among all summaries with the given space usage. But for conjunctions of m\{\textgreater\}2 predicates we show that the performance of b-bit min-wise hashing (and more generally any method based on "k-permutation" min-hash) deteriorates as m grows. We describe a new summary that nearly matches our lower bound for m \{\textgreater\}= 2. It asymptotically outperform all k-permutation schemes (by around a factor Omega(m/log m)), as well as methods based on subsampling (by a factor Omega(log n\_max), where n\_max is the maximum set size).},
  doi={10.1145/2594538.2594554},
  isbn={9781450323758},
  author={Pagh, Rasmus and St{\"o}ckel, Morten and Woodruff, David},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Forecast/Pagh_IsMinwiseHashingOptimal.pdf}
}
@article{Ting_Towards_2016,
  year={2016},
  author={Ting, Daniel},
  doi={10.1145/2939672.2939772},
  title={Towards Optimal Cardinality Estimation of Unions and Intersections with Sketches},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Forecast/Ting_Facebook_OptimalCardinalityUnionIntersection.pdf},
  pages={1195-1204}
}
@book{Ting_Streamed,
  year={0},
  abstract={Counting the number of distinct elements in a large dataset is a common task in web applications and databases. This problem is difficult in limited memory settings where storing a large hash table table is intractable. This paper advances the state of the art in probabilistic methods for estimating the number of distinct elements in a streaming setting New streaming algorithms are given that provably beat the "optimal" errors for Min-count and {HyperLogLog} while using the same sketch. This paper also contributes to the understanding and theory of probabilistic cardinality estimation introducing the concept of an area cutting process and the martingale estimator. These ideas lead to theoretical analyses of both old and new sketches and estimators and show the new estimators are optimal for several streaming settings while also providing accurate error bounds that match those obtained via simulation. Furthermore, the area cutting process provides a geometric intuition behind all methods for counting distinct elements which are not affected by duplicates. This intuition leads to a new sketch, Discrete Max-count, and the analysis of a class of sketches, self-similar area cutting decompositions that have attractive properties and unbiased estimators for both streaming and non-streaming settings. Together, these contributions lead to multi-faceted advances in sketch construction, cardinality and error estimation, the theory, and intuition for the problem of approximate counting of distinct elements for both the streaming and non-streaming cases.},
  doi={10.1145/2623330.2623669},
  isbn={9781450329569},
  author={Ting, Daniel},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Forecast/TingKDD2014_StreamedApproximateCounting.pdf}
}
@article{Aouiche_Unasssuming_2007,
  year={2007},
  author={Aouiche, Kamel and Lemire, Daniel},
  title={Unasssuming {View-Size} Estimation Techniques in {OLAP}},
  abstract={Even if storage was infinite, a data warehouse could not materialize all possible views due to the running time and update requirements. Therefore, it is necessary to estimate quickly, accurately, and reliably the size of views. Many available techniques make particular statistical assumptions and their error can be quite large. Unassuming techniques exist, but typically assume we have independent hashing for which there is no known practical implementation. We adapt an unassuming estimator due to Gibbons and Tirthapura: its theoretical bounds do not make unpractical assumptions. We compare this technique experimentally with stochastic probabilistic counting, {LogLog} probabilistic counting, and multifractal statistical models. Our experiments show that we can reliably and accurately (within 10\%, 19 times out 20) estimate view sizes over large data sets (1.5 {GB)} within minutes, using almost no memory. However, only {Gibbons-Tirthapura} provides universally tight estimates irrespective of the size of the view. For large views, probabilistic counting has a small edge in accuracy, whereas the competitive sampling-based method (multifractal) we tested is an order of magnitude faster but can sometimes provide poor estimates (relative error of 100\%). In our tests, {LogLog} probabilistic counting is not competitive. Experimental validation on the {US} Census 1990 data set and on the Transaction Processing Performance {(TPC} H) data set is provided.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Forecast/UnassumingViewEstimation.pdf}
}
@article{Angrist_Identification_1996,
  year={1996},
  journal={J Am Stat Assoc},
  author={Angrist, Joshua and Imbens, Guido and Rubin, Donald},
  volume={91},
  doi={10.1080/01621459.1996.10476902},
  title={Identification of Causal Effects Using Instrumental Variables},
  number={434},
  issn={0162-1459},
  abstract={Abstract We outline a framework for causal inference in settings where assignment to a binary treatment is ignorable, but compliance with the assignment is not perfect so that the receipt of treatment is nonignorable. To address the problems associated with comparing },
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Lift/AngristImbensRubin_CausalEffectsInstrumentalVariables_TALK.pdf},
  pages={444-455}
}
@article{Barajas_Experimental_2016,
  year={2016},
  journal={Market Sci},
  author={Barajas, Joel and Akella, Ram and Holtan, Marius and Flores, Aaron},
  volume={35},
  doi={10.1287/mksc.2016.0982},
  title={Experimental Designs and Estimation for Online Display Advertising Attribution in Marketplaces},
  number={3},
  issn={0732-2399},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Lift/Barajas_CausalEffectPotentialOutcomes.pdf},
  pages={465-483}
}
@article{Brodersen_Inferring_2015,
  year={2015},
  journal={Ann Appl Statistics},
  author={Brodersen, Kay and Gallusser, Fabian and Koehler, Jim and Remy, Nicolas and Scott, Steven},
  volume={9},
  doi={10.1214/14-AOAS788},
  title={Inferring causal impact using Bayesian structural time-series models},
  number={1},
  issn={1932-6157},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Lift/Brodersen_BayesianTimeSeriesCausality.pdf;/Users/pchalasani/Dropbox/MediaMath/Papers/Lift/Google_CausalIMpactPaper.pdf},
  pages={247-274}
}
@article{Athey_Machine_2015,
  year={2015},
  author={Athey, Susan and Imbens, Guido},
  title={Machine Learning Methods for Estimating Heterogeneous Causal Effects},
  abstract={In this paper we study the problems of estimating heterogeneity in causal effects in experimental or observational studies and conducting inference about the magnitude of the differences in treatment effects across subsets of the population. In applications, our method provides a data-driven approach to determine which subpopulations have large or small treatment effects and to test hypotheses about the differences in these effects. For experiments, our method allows researchers to identify heterogeneity in treatment effects that was not specified in a pre-analysis plan, without concern about invalidating inference due to multiple testing. In most of the literature on supervised machine learning (e.g. regression trees, random forests, {LASSO,} etc.), the goal is to build a model of the relationship between a unit's attributes and an observed outcome. A prominent role in these methods is played by cross-validation which compares predictions to actual outcomes in test samples, in order to select the level of complexity of the model that provides the best predictive power. Our method is closely related, but it differs in that it is tailored for predicting causal effects of a treatment rather than a unit's outcome. The challenge is that the "ground truth" for a causal effect is not observed for any individual unit: we observe the unit with the treatment, or without the treatment, but not both at the same time. Thus, it is not obvious how to use cross-validation to determine whether a causal effect has been accurately predicted. We propose several novel cross-validation criteria for this problem and demonstrate through simulations the conditions under which they perform better than standard methods for the problem of causal effects. We then apply the method to a large-scale field experiment re-ranking results on a search engine.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Lift/Causality/Athey_Imbens_MLForCausalInference.pdf}
}
@article{Barajas_Marketing_2012,
  year={2012},
  journal={Proceedings of the  …},
  author={Barajas, Joel and Kwon, Jaimie and Akella, Ram and Flores, Aaron and Holtan, Marius and Andrei, Victor},
  doi={10.1145/2351356.2351361},
  title={Marketing campaign evaluation in targeted display advertising},
  abstract={Abstract In this paper, we develop an experimental analysis to estimate the causal effect of online marketing campaigns as a whole, and not just the media ad design. We analyze the causal effects on user conversion probability. We run experiments based on {A/B} testing to },
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Lift/Causality/BarajasAkella_MarketingCampaignEvaluation.pdf},
  pages={1-7}
}
@article{Becque_A_2015,
  year={2015},
  journal={Stat Med},
  author={Becque, Taeko and White, Ian and Haggard, Mark},
  volume={34},
  doi={10.1002/sim.6468},
  title={A causal model for longitudinal randomised trials with time-dependent non-compliance.},
  number={12},
  pmid={25778798},
  issn={1097-0258},
  abstract={In the presence of non-compliance, conventional analysis by intention-to-treat provides an unbiased comparison of treatment policies but typically under-estimates treatment efficacy. With all-or-nothing compliance, efficacy may be specified as the complier-average causal effect {(CACE),} where compliers are those who receive intervention if and only if randomised to it. We extend the {CACE} approach to model longitudinal data with time-dependent non-compliance, focusing on the situation in which those randomised to control may receive treatment and allowing treatment effects to vary arbitrarily over time. Defining compliance type to be the time of surgical intervention if randomised to control, so that compliers are patients who would not have received treatment at all if they had been randomised to control, we construct a causal model for the multivariate outcome conditional on compliance type and randomised arm. This model is applied to the trial of alternative regimens for glue ear treatment evaluating surgical interventions in childhood ear disease, where outcomes are measured over five time points, and receipt of surgical intervention in the control arm may occur at any time. We fit the models using Markov chain Monte Carlo methods to obtain estimates of the {CACE} at successive times after receiving the intervention. In this trial, over a half of those randomised to control eventually receive intervention. We find that surgery is more beneficial than control at 6months, with a small but non-significant beneficial effect at 12months.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Lift/Causality/Becque_CausalModelLongitudinalNonCompliance.pdf},
  pages={2019-34}
}
@article{Hfler_Causal_2005,
  year={2005},
  journal={Bmc Med Res Methodol},
  author={H{\"o}fler, M},
  volume={5},
  doi={10.1186/1471-2288-5-28},
  title={Causal inference based on counterfactuals},
  number={1},
  pmid={16159397},
  issn={1471-2288},
  abstract={Background The counterfactual or potential outcome model has become increasingly standard for causal inference in epidemiological and medical studies. Discussion This paper provides an overview on the counterfactual and related approaches. A variety of conceptual as well as practical issues when estimating causal effects are reviewed. These include causal interactions, imperfect experiments, adjustment for confounding, time-varying exposures, competing risks and the probability of causation. It is argued that the counterfactual model of causal effects captures the main aspects of causality in health sciences and relates to many statistical procedures. Summary Counterfactuals are the basis of causal inference in medicine and epidemiology. Nevertheless, the estimation of counterfactual differences pose several difficulties, primarily in observational studies. These problems, however, reflect fundamental barriers only when learning from observations, and this does not invalidate the counterfactual concept.},
  pmcid={{PMC1239917}},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Lift/Causality/CausalInferenceCounterfactuals.pdf},
  pages={1-12}
}
@book{BD314DB5-F05C-34E7-ECB7-2B63F0557D9C,
  year={2013},
  doi={10.1007/978-94-007-6094-3},
  isbn={9789400760936},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Lift/Causality/Elwert 2013_GraphicalCausalModels.pdf}
}
@article{Coey_People_2016,
  year={2016},
  author={Coey, Dominic and Bailey, Michael},
  doi={10.1145/2872427.2882984},
  title={People and Cookies},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Lift/Causality/Facebook_PeopleCookies.pdf},
  pages={1103-1111}
}
@article{Heckman_Econometric_2008,
  year={2008},
  journal={Int Stat Rev},
  author={Heckman, James},
  volume={76},
  doi={10.1111/j.1751-5823.2007.00024.x},
  title={Econometric Causality},
  number={1},
  issn={1751-5823},
  abstract={This paper presents the econometric approach to causal modelling. It is motivated by policy problems. New causal parameters are defined and identified to address specific policy problems. Economists embrace a scientific approach to causality and model the preferences and choices of agents to infer subjective (agent) evaluations as well as objective outcomes. Anticipated and realized subjective and objective outcomes are distinguished. Models for simultaneous causality are developed. The paper contrasts the {Neyman–Rubin} model of causality with the econometric approach.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Lift/Causality/Heckman2008_EconometricCausality.pdf},
  pages={1-27}
}
@article{Johansson_Learning_2016,
  year={2016},
  author={Johansson, Fredrik D and Shalit, Uri and Sontag, David},
  title={Learning Representations for Counterfactual Inference},
  abstract={Observational studies are rising in importance due to the widespread accumulation of data in fields such as healthcare, education, employment and ecology. We consider the task of answering counterfactual questions such as, {"Would} this patient have lower blood sugar had she received a different medication?". We propose a new algorithmic framework for counterfactual inference which brings together ideas from domain adaptation and representation learning. In addition to a theoretical justification, we perform an empirical comparison with previous approaches to causal inference from observational data. Our deep learning algorithm significantly outperforms the previous state-of-the-art.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Lift/Causality/SontagShalit_ICML16_LearningReprCounterfactualInference.pdf}
}
@article{Lewis_The_2015,
  year={2015},
  journal={Q J Econ},
  author={Lewis, Randall and Rao, Justin},
  volume={130},
  doi={10.1093/qje/qjv023},
  title={The Unfavorable Economics of Measuring the Returns to Advertising},
  number={4},
  issn={0033-5533},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Lift/Causality/LewisRao_UnfavorableEconomicsMeasuring.pdf},
  pages={1941-1973}
}
@article{Matsuyama_Correcting_2002,
  year={2002},
  journal={Stat Med},
  author={Matsuyama, Yutaka},
  volume={21},
  doi={10.1002/sim.1002},
  title={Correcting for non‐compliance of repeated binary outcomes in randomized clinical trials: randomized analysis approach},
  number={5},
  pmid={11870809},
  issn={1097-0258},
  abstract={We develop the randomized analysis for repeated binary outcomes with non-compliance. A break randomization-based semi-parametric estimation procedure for both the causal risk difference and the causal risk ratio is proposed for repeated binary data. Although we assume the simple structural models for potential outcomes, we choose to avoid making any assumptions about comparability beyond those implied by randomization at time zero. The proposed methods can incorporate non-compliance information, while preserving the validity of the test of the null hypothesis, and even in the presence of non-random non-compliance can give the estimate of the causal effect that treatment would have if all individuals complied with their assigned treatment. The methods are applied to data from a randomized clinical trial for reduction of febrile neutropenia events among acute myeloid leukaemia patients, in which a prophylactic use of macrophage colony-stimulating factor {(M-CSF)} was compared to placebo during the courses of intensive chemotherapies. Copyright {\textcopyright} 2002 John Wiley \& Sons, Ltd.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Lift/Causality/Matsuyama2002_CorrectingNonComplianceRepeatedTrials.pdf},
  pages={675-687}
}
@article{Tingley_mediation_2014,
  year={2014},
  journal={J Stat Softw},
  author={Tingley, Dustin and Yamamoto, Teppei and Hirose, Kentaro and Keele, Luke and Imai, Kosuke},
  volume={59},
  doi={10.18637/jss.v059.i05},
  title={{mediation:RPackage} for Causal Mediation Analysis},
  number={5},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Lift/Causality/R_pkg_mediation_causal.pdf}
}
@article{Rubin_Causal_2005,
  year={2005},
  journal={J Am Stat Assoc},
  author={Rubin, Donald},
  volume={100},
  doi={10.1198/016214504000001880},
  title={Causal Inference Using Potential Outcomes},
  number={469},
  pmid={6943859},
  issn={0162-1459},
  abstract={Causal effects are defined as comparisons of potential outcomes under different treatments on a common set of units. Observed values of the potential outcomes are revealed by the assignment mechanism—a probabilistic model for the treatment each unit receives as a },
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Lift/Causality/Rubin-JASA-05.pdf},
  pages={322-331}
}
@article{Rubin_For_2008,
  year={2008},
  journal={Ann Appl Statistics},
  author={Rubin, Donald},
  volume={2},
  doi={10.1214/08-AOAS187},
  title={For objective causal inference, design trumps analysis},
  number={3},
  issn={1932-6157},
  abstract={For obtaining causal inferences that are objective, and therefore have the best chance of revealing scientific truths, carefully designed and executed randomized experiments are generally considered to be the gold standard. Observational studies, in contrast, are generally fraught with problems that compromise any claim for objectivity of the resulting causal inferences. The thesis here is that observational studies have to be carefully designed to approximate randomized experiments, in particular, without examining any final outcome data. Often a candidate data set will have to be rejected as inadequate because of lack of data on key covariates, or because of lack of overlap in the distributions of key covariates between treatment and control groups, often revealed by careful propensity score analyses. Sometimes the template for the approximating randomized experiment will have to be altered, and the use of principal stratification can be helpful in doing this. These issues are discussed and illustrated using the framework of potential outcomes to define causal effects, which greatly clarifies critical issues.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Lift/Causality/Rubin2011_CausalityDesignTrumpsAnalysis.pdf},
  pages={808-840}
}
@article{Toh_Causal_2008,
  year={2008},
  journal={Int J Biostat},
  author={Toh, Sengwee and Hern{\'a}n, Miguel},
  volume={4},
  doi={10.2202/1557-4679.1117},
  title={Causal inference from longitudinal studies with baseline randomization.},
  number={1},
  pmid={20231914},
  abstract={We describe analytic approaches for study designs that, like large simple trials, can be better characterized as longitudinal studies with baseline randomization than as either a pure randomized experiment or a purely observational study. We (i) discuss the intention-to-treat effect as an effect measure for randomized studies, (ii) provide a formal definition of causal effect for longitudinal studies, (iii) describe several methods -- based on inverse probability weighting and g-estimation -- to estimate such effect, (iv) present an application of these methods to a naturalistic trial of antipsychotics on symptom severity of schizophrenia, and (v) discuss the relative advantages and disadvantages of each method.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Lift/Causality/Toh_CausalInferenceLogitudinal.pdf},
  pages={Article 22}
}
@book{Chan_Evaluating,
  year={2010},
  abstract={Display ads proliferate on the web, but are they effective? Or are they irrelevant in light of all the other advertising that people see? We describe a way to answer these questions, quickly and accurately, without randomized experiments, surveys, focus groups or expert data analysts. Doubly robust estimation protects against the selection bias that is inherent in observational data, and a nonparametric test that is based on irrelevant outcomes provides further defense. Simulations based on realistic scenarios show that the resulting estimates are more robust to selection bias than traditional alternatives, such as regression modeling or propensity scoring. Moreover, computations are fast enough that all processing, from data retrieval through estimation, testing, validation and report generation, proceeds in an automated pipeline, without anyone needing to see the raw data.},
  doi={10.1145/1835804.1835809},
  title={Evaluating online ad campaigns in a pipeline: causal models at scale},
  isbn={9781450300551},
  author={Chan, David and Ge, Rong and Gershony, Ori and Hesterberg, Tim and Lambert, Diane},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Lift/Google_EvaluatingAdsPipelineCausal.pdf}
}
@article{Lewis_Online_2014,
  year={2014},
  journal={Quantitative Mark Econ},
  author={Lewis, Randall and Reiley, David},
  volume={12},
  doi={10.1007/s11129-014-9146-6},
  title={Online ads and offline sales: measuring the effect of retail advertising via a controlled experiment on Yahoo!},
  number={3},
  issn={1570-7156},
  abstract={A randomized experiment with 1.6 million customers measures positive causal effects of online advertising for a major retailer. The advertising profitably increases purchases by 5\%. 93\% of the increase occurs in brick-and-mortar stores; 78\% of the increase derives from consumers who never click the ads. Our large sample reaches the statistical frontier for measuring economically relevant effects. We improve econometric efficiency by supplementing our experimental variation with non-experimental variation caused by consumer browsing behavior. Our experiment provides a specification check for observational difference-in-differences and cross-sectional estimators; the latter exhibits a large negative bias three times the estimated experimental effect.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Lift/JohnsonLewis_GhostAds.pdf},
  pages={235-266}
}
@article{Little_Causal_2000,
  year={2000},
  journal={Annu Rev Publ Health},
  author={Little, R and Rubin, D},
  volume={21},
  doi={10.1146/annurev.publhealth.21.1.121},
  title={Causal effects in clinical and epidemiological studies via potential outcomes: concepts and analytical approaches.},
  number={1},
  pmid={10884949},
  issn={0163-7525},
  abstract={A central problem in public health studies is how to make inferences about the causal effects of treatments or agents. In this article we review an approach to making such inferences via potential outcomes. In this approach, the causal effect is defined as a comparison of results from two or more alternative treatments, with only one of the results actually observed. We discuss the application of this approach to a number of data collection designs and associated problems commonly encountered in clinical research and epidemiology. Topics considered include the fundamental role of the assignment mechanism, in particular the importance of randomization as an unconfounded method of assignment; randomization-based and model-based methods of statistical inference for causal effects; methods for handling noncompliance and missing data; and methods for limiting bias in the analysis of observational data, including propensity score matching and sensitivity analysis.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Lift/Little_Rubin_2000_CausalAnalysis.pdf},
  pages={121-45}
}
@article{Little_A_2009,
  year={2009},
  journal={Biometrics},
  author={Little, Roderick and Long, Qi and Lin, Xihong},
  volume={65},
  doi={10.1111/j.1541-0420.2008.01066.x},
  title={A Comparison of Methods for Estimating the Causal Effect of a Treatment in Randomized Clinical Trials Subject to Noncompliance},
  number={2},
  pmid={18510650},
  issn={1541-0420},
  abstract={Summary We consider the analysis of clinical trials that involve randomization to an active treatment {(T} = 1) or a control treatment {(T} = 0), when the active treatment is subject to all-or-nothing compliance. We compare three approaches to estimating treatment efficacy in this situation: as-treated analysis, per-protocol analysis, and instrumental variable {(IV)} estimation, where the treatment effect is estimated using the randomization indicator as an {IV.} Both model- and method-of-moment based {IV} estimators are considered. The assumptions underlying these estimators are assessed, standard errors and mean squared errors of the estimates are compared, and design implications of the three methods are examined. Extensions of the methods to include observed covariates are then discussed, emphasizing the role of compliance propensity methods and the contrasting role of covariates in these extensions. Methods are illustrated on data from the Women Take Pride study, an assessment of behavioral treatments for women with heart disease.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Lift/littlelonglin_Survey_CausalEffectNonCompliance_biometrics2009.pdf},
  pages={640-649}
}
@article{Head_The_2015,
  year={2015},
  journal={Plos Biol},
  author={Head, Megan and Holman, Luke and Lanfear, Rob and Kahn, Andrew and Jennions, Michael},
  volume={13},
  doi={10.1371/journal.pbio.1002106},
  title={The Extent and Consequences of {P-Hacking} in Science},
  number={3},
  pmid={25768323},
  issn={1544-9173},
  abstract={A focus on novel, confirmatory, and statistically significant results leads to substantial bias in the scientific literature. One type of bias, known as “p-hacking,” occurs when researchers collect or select data or statistical analyses until nonsignificant results become significant. Here, we use text-mining to demonstrate that p-hacking is widespread throughout science. We then illustrate how one can test for p-hacking when performing a meta-analysis and show that, while p-hacking is probably common, its effect seems to be weak relative to the real effect sizes being measured. This result suggests that p-hacking probably does not drastically alter scientific consensuses drawn from meta-analyses.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Lift/p-hacking-extent.pdf},
  pages={e1002106}
}
@book{Wang_An_2014,
  year={2014},
  abstract={In online advertising market it is crucial to provide advertisers with a reliable measurement of advertising effectiveness to make better marketing campaign planning. The basic idea for ad effectiveness measurement is to compare the performance (e.g., success rate) among users who were and who were not exposed to a certain treatment of ads. When a randomized experiment is not available, a naive comparison can be biased because exposed and unexposed populations typically have different features. One solid methodology for a fair comparison is to apply inverse propensity weighting with doubly robust estimation to the observational data. However the existing methods were not designed for the online advertising campaign, which usually suffers from huge volume of users, high dimensionality, high sparsity and imbalance. We propose an efficient framework to address these challenges in a real campaign circumstance. We utilize gradient boosting stumps for feature selection and gradient boosting trees for model fitting, and propose a subsampling-and-backscaling procedure that enables analysis on extremely sparse conversion data. The choice of features, models and feature selection scheme are validated with irrelevant conversion test. We further propose a parallel computing strategy, combined with the subsampling-and-backscaling procedure to reach computational efficiency. Our framework is applied to an online campaign involving millions of unique users, which shows substantially better model fitting and efficiency. Our framework can be further generalized to comparison of multiple treatments and more general treatment regimes, as sketched in the paper. Our framework is not limited to online advertising, but also applicable to other circumstances (e.g., social science) where a 'fair' comparison is needed with observational data.},
  doi={10.1145/2556195.2556235},
  isbn={9781450323512},
  author={Wang, Pengyuan and Liu, Yechao and Meytlis, Marsha and Tsao, {Han-Yun} and Yang, Jian and Huang, Pei},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Lift/Pengyuang_UnifiedFrameworkAdImpact.pdf}
}
@article{Plamadeala_Sequential_2012,
  year={2012},
  journal={Ann Statistics},
  author={Plamadeala, Victoria and Rosenberger, William},
  volume={40},
  doi={10.1214/11-AOS941},
  title={Sequential monitoring with conditional randomization tests},
  number={1},
  issn={0090-5364},
  abstract={Sequential monitoring in clinical trials is often employed to allow for early stopping and other interim decisions, while maintaining the type I error rate. However, sequential monitoring is typically described only in the context of a population model. We describe a computational method to implement sequential monitoring in a randomization-based context. In particular, we discuss a new technique for the computation of approximate conditional tests following restricted randomization procedures and then apply this technique to approximate the joint distribution of sequentially computed conditional randomization tests. We also describe the computation of a randomization-based analog of the information fraction. We apply these techniques to a restricted randomization procedure, Efron's {[Biometrika} 58 (1971) 403--417] biased coin design. These techniques require derivation of certain conditional probabilities and conditional covariances of the randomization procedure. We employ combinatoric techniques to derive these for the biased coin design.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Lift/Plamadeala_SequentialMonitoring.pdf},
  pages={30-44}
}
@article{Greist_Chapter_1987,
  year={1987},
  journal={Comput Hum Serv},
  author={Greist, John and Carroll, Judith and Erdman, Harold and Klein, Marjorie and Wurster, Cecil},
  volume={2},
  doi={10.1300/J407v02n03_10},
  title={Chapter 10},
  number={3-4},
  issn={{0740-445X}},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Lift/Stat-520-course-compliance-chapter_10.pdf},
  pages={171-176}
}
@article{Chapelle_Modeling_2014,
  year={2014},
  author={Chapelle, Olivier},
  doi={10.1145/2623330.2623634},
  title={Modeling delayed feedback in display advertising},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/ML/Chapelle_delayed-feedback.pdf},
  pages={1097-1105}
}
@book{Chapelle_Offline,
  year={0},
  abstract={Click-through rates and conversion rates are two core machine learning problems in online advertising. The evaluation of such systems is often based on traditional supervised learning metrics that ignore how the predictions are used. These predictions are in fact part of bidding systems in online advertising auctions. We present here an empirical evaluation of a metric that is specifically tailored for auctions in online advertising and show that it correlates better than standard metrics with {A/B} test results.},
  doi={10.1145/2740908.2742566},
  title={Offline Evaluation of Response Prediction in Online Advertising Auctions},
  isbn={9781450334730},
  author={Chapelle, Olivier},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/ML/Chapelle_OfflineEvaluationMetrics.pdf;/Users/pchalasani/Documents/ReadCube Media/Chapelle.pdf}
}
@article{Agarwal_A_2011,
  year={2011},
  author={Agarwal, Alekh and Chapelle, Olivier and Dudik, Miroslav and Langford, John},
  title={A Reliable Effective Terascale Linear Learning System},
  abstract={We present a system and a set of techniques for learning linear predictors with convex losses on terascale datasets, with trillions of features, {\{The} number of features here refers to the number of non-zero entries in the data matrix.\} billions of training examples and millions of parameters in an hour using a cluster of 1000 machines. Individually none of the component techniques are new, but the careful synthesis required to obtain an efficient implementation is. The result is, up to our knowledge, the most scalable and efficient linear learning system reported in the literature (as of 2011 when our experiments were conducted). We describe and thoroughly evaluate the components of the system, showing the importance of the various design choices.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/ML/AgarwalLangford_ReliableTerascaleLearning.pdf}
}
@article{Langford_Sparse_2008,
  year={2008},
  author={Langford, John and Li, Lihong and Zhang, Tong},
  title={Sparse Online Learning via Truncated Gradient},
  abstract={We propose a general method called truncated gradient to induce sparsity in the weights of online learning algorithms with convex loss functions. This method has several essential properties: The degree of sparsity is continuous -- a parameter controls the rate of sparsification from no sparsification to total sparsification. The approach is theoretically motivated, and an instance of it can be regarded as an online counterpart of the popular {\$L\_1\$-regularization} method in the batch setting. We prove that small rates of sparsification result in only small additional regret with respect to typical online learning guarantees. The approach works well empirically. We apply the approach to several datasets and find that for datasets with large numbers of features, substantial sparsity is discoverable.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/ML/Langford_SparseTruncatedGradient.pdf}
}
@article{Bao_Simultaneously_2014,
  year={2014},
  author={Bao, Yang and Datta, Anindya},
  volume={60},
  doi={10.1287/mnsc.2014.1930},
  title={Simultaneously Discovering and Quantifying Risk Types from Textual Risk Disclosures},
  number={6},
  issn={0025-1909},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/MobileWalla/BaoYang_LSA.pdf},
  pages={1371-1391}
}
@article{Ross_Normalized_2014,
  year={2014},
  author={Ross, Stephane and Mineiro, Paul and Langford, John},
  title={Normalized Online Learning},
  abstract={We introduce online learning algorithms which are independent of feature scales, proving regret bounds dependent on the ratio of scales existent in the data rather than the absolute scale. This has several useful effects: there is no need to pre-normalize data, the test-time and test-space complexity are reduced, and the algorithms are more robust.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/ML/VW_normalized_gradient.pdf}
}
@article{Schaul_No_2012,
  year={2012},
  author={Schaul, Tom and Zhang, Sixin and {LeCun,} Yann},
  title={No More Pesky Learning Rates},
  abstract={The performance of stochastic gradient descent {(SGD)} depends critically on how learning rates are tuned and decreased over time. We propose a method to automatically adjust multiple learning rates so as to minimize the expected error at any one time. The method relies on local gradient variations across samples. In our approach, learning rates can increase as well as decrease, making it suitable for non-stationary problems. Using a number of convex and non-convex learning tasks, we show that the resulting algorithm matches the performance of {SGD} or other adaptive approaches with their best settings obtained through systematic search, and effectively removes the need for learning rate tuning.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/ML/NoMorePeskyLearningRates.pdf}
}
@article{McMahan_A_2014,
  year={2014},
  author={{McMahan,} H},
  title={A Survey of Algorithms and Analysis for Adaptive Online Learning},
  abstract={We present tools for the analysis of {Follow-The-Regularized-Leader} {(FTRL),} Dual Averaging, and Mirror Descent algorithms when the regularizer (equivalently, prox-function or learning rate schedule) is chosen adaptively based on the data. Adaptivity can be used to prove regret bounds that hold on every round, and also allows for data-dependent regret bounds as in {AdaGrad-style} algorithms (e.g., Online Gradient Descent with adaptive per-coordinate learning rates). We present results from a large number of prior works in a unified manner, using a modular and tight analysis that isolates the key arguments in easily re-usable lemmas. This approach strengthens pre-viously known {FTRL} analysis techniques to produce bounds as tight as those achieved by potential functions or primal-dual analysis. Further, we prove a general and exact equivalence between an arbitrary adaptive Mirror Descent algorithm and a correspond- ing {FTRL} update, which allows us to analyze any Mirror Descent algorithm in the same framework. The key to bridging the gap between Dual Averaging and Mirror Descent algorithms lies in an analysis of the {FTRL-Proximal} algorithm family. Our regret bounds are proved in the most general form, holding for arbitrary norms and non-smooth regularizers with time-varying weight.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/ML/McMahan_SurveyOnlineAdaptiveLearning.pdf}
}
@book{Geyik_Joint,
  year={0},
  abstract={The field of online advertising, in essence, deals with the problem of presenting ads to online users in the most appropriate contexts to achieve a multitude of advertiser goals. A vast amount of work in online advertising has been focused on optimizing banner display advertising campaigns where the main goal lies in direct response metrics, often as clicks or conversions. In this paper, we explore the newly popularized space of online video advertising, where brand recognition is the key focus. We propose a framework based on a feedback mechanism where we optimize multiple video specific performance indicators while making sure the delivery constraints (budget and user reach) of advertisers are satisfied. While our main focus is on improving metrics such as engagement (amount of view time), and viewability (whether a campaign is within eyesight of a user), we also discuss the possibilities of expanding to other metrics. We demonstrate the benefit of our framework via empirical results in multiple real-world advertising campaigns. To the best of our knowledge, this is the first paper that deals with the unique challenges arising from the nature of online video advertising.},
  doi={10.1145/2939672.2939724},
  isbn={9781450342322},
  author={Geyik, Sahin and Faleev, Sergey and Shen, Jianqiang and {O'Donnell,} Sean and Kolay, Santanu},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/MultiGoalOpt/Turn_MultiObjectiveVideoAds.pdf}
}
@article{Arora_Simple_2015,
  year={2015},
  author={Arora, Sanjeev and Ge, Rong and Ma, Tengyu and Moitra, Ankur},
  title={Simple, Efficient, and Neural Algorithms for Sparse Coding},
  abstract={Sparse coding is a basic task in many fields including signal processing, neuroscience and machine learning where the goal is to learn a basis that enables a sparse representation of a given set of data, if one exists. Its standard formulation is as a non-convex optimization problem which is solved in practice by heuristics based on alternating minimization. Re- cent work has resulted in several algorithms for sparse coding with provable guarantees, but somewhat surprisingly these are outperformed by the simple alternating minimization heuristics. Here we give a general framework for understanding alternating minimization which we leverage to analyze existing heuristics and to design new ones also with provable guarantees. Some of these algorithms seem implementable on simple neural architectures, which was the original motivation of Olshausen and Field (1997a) in introducing sparse coding. We also give the first efficient algorithm for sparse coding that works almost up to the information theoretic limit for sparse recovery on incoherent dictionaries. All previous algorithms that approached or surpassed this limit run in time exponential in some natural parameter. Finally, our algorithms improve upon the sample complexity of existing approaches. We believe that our analysis framework will have applications in other settings where simple iterative algorithms are used.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Sparse/Arora_NeuralSparsePaper.pdf}
}
@article{Cheng_Wide_2016,
  year={2016},
  author={Cheng, {Heng-Tze} and Koc, Levent and Harmsen, Jeremiah and Shaked, Tal and Chandra, Tushar and Aradhye, Hrishi and Anderson, Glen and Corrado, Greg and Chai, Wei and Ispir, Mustafa and Anil, Rohan and Haque, Zakaria and Hong, Lichan and Jain, Vihan and Liu, Xiaobing and Shah, Hemal},
  title={Wide \& Deep Learning for Recommender Systems},
  abstract={Generalized linear models with nonlinear feature transformations are widely used for large-scale regression and classification problems with sparse inputs. Memorization of feature interactions through a wide set of cross-product feature transformations are effective and interpretable, while generalization requires more feature engineering effort. With less feature engineering, deep neural networks can generalize better to unseen feature combinations through low-dimensional dense embeddings learned for the sparse features. However, deep neural networks with embeddings can over-generalize and recommend less relevant items when the user-item interactions are sparse and high-rank. In this paper, we present Wide \& Deep learning---jointly trained wide linear models and deep neural networks---to combine the benefits of memorization and generalization for recommender systems. We productionized and evaluated the system on Google Play, a commercial mobile app store with over one billion active users and over one million apps. Online experiment results show that Wide \& Deep significantly increased app acquisitions compared with wide-only and deep-only models. We have also open-sourced our implementation in {TensorFlow.}},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Deep/Google_WideDeepLearning.pdf}
}
@book{Lin_Combining_2016,
  year={2016},
  abstract={We address the bidding strategy design problem faced by a {Demand-Side} Platform {(DSP)} in {Real-Time} Bidding {(RTB)} advertising. A {RTB} campaign consists of various parameters and usually a predefined budget. Under the budget constraint of a campaign, designing an optimal strategy for bidding on each impression to acquire as many clicks as possible is a main job of a {DSP.} State-of-the-art bidding algorithms rely on a single predictor, namely the clickthrough rate {(CTR)} predictor, to calculate the bidding value for each impression. This provides reasonable performance if the predictor has appropriate accuracy in predicting the probability of user clicking. However when the predictor gives only moderate accuracy, classical algorithms fail to capture optimal results. We improve the situation by accomplishing an additional winning price predictor in the bidding process. In this paper, a method combining powers of two prediction models is proposed, and experiments with real world {RTB} datasets from benchmarking the new algorithm with a classic {CTR-only} method are presented. The proposed algorithm performs better with regard to both number of clicks achieved and effective cost per click in many different settings of budget constraints.},
  doi={10.1145/2983323.2983656},
  isbn={9781450340731},
  author={Lin, {Chi-Chun} and Chuang, {Kun-Ta} and Wu, Wush and Chen, {Ming-Syan}}
}
@article{Shalit_Estimating_2016,
  year={2016},
  author={Shalit, Uri and Johansson, Fredrik and Sontag, David},
  title={Estimating individual treatment effect: generalization bounds and algorithms},
  abstract={There is intense interest in applying machine learning to problems of causal inference in healthcare, economics, education, and other fields. In particular, individual-level causal inference has applications such as precision medicine and personalized advertising. We give a new theoretical analysis and family of algorithms for estimating individual treatment effect {(ITE)} from observational data. The algorithm itself learns a "balanced" representation such that the induced treated and control distributions look similar. We give a novel, simple and intuitive generalization error bound showing that the expected {ITE} estimation error of a representation is bounded by a sum of the standard generalization error of that representation and the distance between the treated and control distributions induced by the representation. We use Integral Probability Metrics to measure distances between distributions, deriving explicit bounds for the Wasserstein and Maximum Mean Discrepancy {(MMD)} distance. Experiments on real and simulated data show the new algorithms match or outperform state-of-the-art methods.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Causal/SontagShalit_IndividualTreatmentEffect.pdf}
}
@article{Athey_Recursive_2016,
  year={2016},
  author={Athey, Susan and Imbens, Guido},
  volume={113},
  doi={10.1073/pnas.1510489113},
  title={Recursive partitioning for heterogeneous causal effects},
  number={27},
  pmid={27382149},
  issn={0027-8424},
  abstract={In this paper we propose methods for estimating heterogeneity in causal effects in experimental and observational studies and for conducting hypothesis tests about the magnitude of differences in treatment effects across subsets of the population. We provide a data-driven approach to partition the data into subpopulations that differ in the magnitude of their treatment effects. The approach enables the construction of valid confidence intervals for treatment effects, even with many covariates relative to the sample size, and without “sparsity” assumptions. We propose an “honest” approach to estimation, whereby one sample is used to construct the partition and another to estimate treatment effects for each subpopulation. Our approach builds on regression tree methods, modified to optimize for goodness of fit in treatment effects and to account for honest estimation. Our model selection criterion anticipates that bias will be eliminated by honest estimation and also accounts for the effect of making additional splits on the variance of treatment effect estimates within each subpopulation. We address the challenge that the “ground truth” for a causal effect is not observed for any individual unit, so that standard approaches to cross-validation must be modified. Through a simulation study, we show that for our preferred method honest estimation results in nominal coverage for 90\% confidence intervals, whereas coverage ranges between 74\% and 84\% for nonhonest approaches. Honest estimation requires estimating the model with a smaller sample size; the cost in terms of mean squared error of treatment effects for our preferred method ranges between 7–22\%.},
  file={/Users/pchalasani/Documents/ReadCube Media/Athey et al-2016-Proc Natl Acad Sci 1.pdf;/Users/pchalasani/Documents/ReadCube Media/Athey et al-2016-Proc Natl Acad Sci - supplement 1.pdf},
  pages={7353-7360}
}
@article{Perlich_Machine_2013,
  year={2013},
  journal={Mach Learn},
  author={Perlich, C and Dalessandro, B and Raeder, T and Stitelman, O and Provost, F},
  volume={95},
  doi={10.1007/s10994-013-5375-2},
  title={Machine learning for targeted display advertising: transfer learning in action},
  number={1},
  issn={0885-6125},
  abstract={This paper presents the design of a fully deployed multistage transfer learning system for targeted display advertising, highlighting the important role of problem formulation and the sampling of data from distributions different from that of the target environment. Notably, the machine learning system itself is deployed and has been in continual use for years for thousands of advertising campaigns—in contrast to the more common case where predictive models are built outside the system, curated, and then deployed. In this domain, acquiring sufficient data for training from the ideal sampling distribution is prohibitively expensive. Instead, data are drawn from surrogate distributions and learning tasks, and then transferred to the target task. We present the design of the transfer learning system We then present a detailed experimental evaluation, showing that the different transfer stages indeed each add value. We also present production results across a variety of advertising clients from a variety of industries, illustrating the performance of the system in use. We close the paper with a collection of lessons learned from over half a decade of research and development on this complex, deployed, and intensely used machine learning system.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Causal/Perlich_MLAdvertisingTransferLearning.pdf},
  pages={103-127}
}
@book{Unknown_Causally_2012,
  year={2012},
  abstract={In many online advertising campaigns, multiple vendors, publishers or search engines (herein called channels) are contracted to serve advertisements to internet users on behalf of a client seeking specific types of conversion. In such campaigns, individual users are often served advertisements by more than one channel. The process of assigning conversion credit to the various channels is called "attribution," and is a subject of intense interest in the industry. This paper presents a causally motivated methodology for conversion attribution in online advertising campaigns. We discuss the need for the standardization of attribution measurement and offer three guiding principles to contribute to this standardization. Stemming from these principles, we position attribution as a causal estimation problem and then propose two approximation methods as alternatives for when the full causal estimation can not be done. These approximate methods derive from our causal approach and incorporate prior attribution work in cooperative game theory. We argue that in cases where causal assumptions are violated, these approximate methods can be interpreted as variable importance measures. Finally, we show examples of attribution measurement on several online advertising campaign data sets.},
  doi={10.1145/2351356.2351363},
  isbn={9781450315456},
  author={Unknown, Unknow and Dalessandro, Brian and Perlich, Claudia and Stitelman, Ori and Provost, Foster},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Causal/Perlich_CausallyMotivatedAttribution.pdf}
}
@book{Raeder_Design_2012,
  year={2012},
  abstract={Most data mining research is concerned with building high-quality classification models in isolation. In massive production systems, however, the ability to monitor and maintain performance over time while growing in size and scope is equally important. Many external factors may degrade classification performance including changes in data distribution, noise or bias in the source data, and the evolution of the system itself. A well-functioning system must gracefully handle all of these. This paper lays out a set of design principles for large-scale autonomous data mining systems and then demonstrates our application of these principles within the m6d automated ad targeting system. We demonstrate a comprehensive set of quality control processes that allow us monitor and maintain thousands of distinct classification models automatically, and to add new models, take on new data, and correct poorly-performing models without manual intervention or system disruption.},
  doi={10.1145/2339530.2339740},
  title={Design principles of massive, robust prediction systems},
  isbn={9781450314626},
  author={Raeder, Troy and Stitelman, Ori and Dalessandro, Brian and Perlich, Claudia and Provost, Foster},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/ML/PerlichProvost_DesignPrinciplesMassivePrediction.pdf}
}
@article{Xu_Smart_2015,
  year={2015},
  author={Xu, Jian and Lee, Kuang-chih and Li, Wentong and Qi, Hang and Lu, Quan},
  doi={10.1145/2783258.2788615},
  title={Smart Pacing for Effective Online Ad Campaign Optimization},
  abstract={In targeted online advertising, advertisers look for maximizing campaign performance under delivery constraint within budget schedule. Most of the advertisers typically prefer to impose the delivery constraint to spend budget smoothly over the time in order to reach a wider range of audiences and have a sustainable impact. Since lots of impressions are traded through public auctions for online advertising today, the liquidity makes price elasticity and bid landscape between demand and supply change quite dynamically. Therefore, it is challenging to perform smooth pacing control and maximize campaign performance simultaneously. In this paper, we propose a smart pacing approach in which the delivery pace of each campaign is learned from both offline and online data to achieve smooth delivery and optimal performance goals. The implementation of the proposed approach in a real {DSP} system is also presented. Experimental evaluations on both real online ad campaigns and offline simulations show that our approach can effectively improve campaign performance and achieve delivery goals.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Bidding/Yahoo_SmartPacingOnlineAds.pdf}
}
@article{Balseiro_Yield_2011,
  year={2011},
  author={Balseiro, Santiago and Feldman, Jon and Mirrokni, Vahab and Muthukrishnan, S},
  title={Yield Optimization of Display Advertising with Ad Exchange},
  abstract={In light of the growing market of Ad Exchanges for the real-time sale of advertising slots, publishers face new challenges in choosing between the allocation of contract-based reservation ads and spot market ads. In this setting, the publisher should take into account the tradeoff between short-term revenue from an Ad Exchange and quality of allocating reservation ads. In this paper, we formalize this combined optimization problem as a stochastic control problem and derive an efficient policy for online ad allocation in settings with general joint distribution over placement quality and exchange bids. We prove asymptotic optimality of this policy in terms of any trade-off between quality of delivered reservation ads and revenue from the exchange, and provide a rigorous bound for its convergence rate to the optimal policy. We also give experimental results on data derived from real publisher inventory, showing that our policy can achieve any pareto-optimal point on the quality vs. revenue curve. Finally, we study a parametric training-based algorithm in which instead of learning the dual variables from a sample data (as is done in non-parametric training-based algorithms), we learn the parameters of the distribution and construct those dual variables from the learned parameter values. We compare parametric and non-parametric ways to estimate from data both analytically and experimentally in the special case without the ad exchange, and show that though both methods converge to the optimal policy as the sample size grows, our parametric method converges faster, and thus performs better on smaller samples.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Mendeley/Balseiro - 2011 - Yield Optimization of Display Advertising with Ad Exchange.pdf}
}
@article{Bell_Game_1988,
  year={1988},
  journal={Manage Sci},
  author={Bell, Robert and Cover, Thomas},
  volume={34},
  doi={10.1287/mnsc.34.6.724},
  title={{Game-Theoretic} Optimal Portfolios},
  number={6},
  issn={0025-1909},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Mendeley/Bell, Cover - 1988 - No Title.pdf},
  pages={724-733}
}
@article{Benaglia_mixtools_2009,
  year={2009},
  journal={J Stat Softw},
  author={Benaglia, Tatiana and Chauveau, Didier and Hunter, David and Young, Derek},
  volume={32},
  doi={10.18637/jss.v032.i06},
  title={mixtools: {AnRPackage} for Analyzing Finite Mixture Models},
  number={6},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Mendeley/Benaglia, Hunter, Young - Unknown - mixtools An R Package for Analyzing Finite Mixture Models.pdf}
}
@article{Bertsimas_Optimal_1998,
  year={1998},
  journal={J Financ Mark},
  author={Bertsimas, Dimitris and Lo, Andrew},
  volume={1},
  doi={10.1016/S1386-4181(97)00012-8},
  title={Optimal control of execution costs},
  number={1},
  issn={1386-4181},
  abstract={We derive dynamic optimal trading strategies that minimize the expected cost of trading a large block of equity over a fixed time horizon. Specifically, given a fixed block S̄ of shares to be executed within a fixed finite number of periods T, and given a price-impact function that yields the execution price of an individual trade as a function of the shares traded and market conditions, we obtain the optimal sequence of trades as a function of market conditions – closed-form expressions in some cases – that minimizes the expected cost of executing S̄ within T periods. Our analysis is extended to the portfolio case in which price impact across stocks can have an important effect on the total cost of trading a portfolio.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Mendeley/Bertsimas, Lo - 1998 - Optimal control of execution costs.pdf},
  pages={1-50}
}
@article{Beverly_Forensic_2011,
  year={2011},
  journal={Digit Invest},
  author={Beverly, Robert and Garfinkel, Simson and Cardwell, Greg},
  volume={8},
  doi={10.1016/j.diin.2011.05.010},
  title={Forensic carving of network packets and associated data structures},
  issn={1742-2876},
  abstract={Using validated carving techniques, we show that popular operating systems (e.g. Windows, Linux, and {OSX)} frequently have residual {IP} packets, Ethernet frames, and associated data structures present in system memory from long-terminated network traffic. Such information is useful for many forensic purposes including establishment of prior connection activity and services used; identification of other systems present on the system’s {LAN} or {WLAN;} geolocation of the host computer system; and cross-drive analysis. We show that network structures can also be recovered from memory that is persisted onto a mass storage medium during the course of system swapping or hibernation. We present our network carving techniques, algorithms and tools, and validate these against both purpose-built memory images and a readily available forensic corpora. These techniques are valuable to both forensics tasks, particularly in analyzing mobile devices, and to cyber-security objectives such as malware analysis.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Mendeley/Beverly, Garfinkel, Cardwell - 2011 - Forensic carving of network packets and associated data structures.pdf},
  pages={S78-S89}
}
@article{Hyvrinen_Connection_2003,
  year={2003},
  journal={Neurocomputing},
  author={Hyv{\"a}rinen, Aapo and Bingham, Ella},
  volume={50},
  doi={10.1016/S0925-2312(01)00705-6},
  title={Connection between multilayer perceptrons and regression using independent component analysis},
  issn={0925-2312},
  abstract={The data model of independent component analysis {(ICA)} gives a multivariate probability density that describes many kinds of sensory data better than classical models like Gaussian densities or Gaussian mixtures. When only a subset of the random variables is observed, {ICA} can be used for regression, i.e. to predict the missing observations. In this paper, we show that the resulting regression is closely related to regression by a multi-layer perceptron {(MLP).} In fact, if linear dependencies are first removed from the data, regression by {ICA} is, as a first-order approximation, equivalent to regression by {MLP.} This theoretical result gives a new interpretation of the elements of the {MLP:} The outputs of the hidden layer neurons are related to estimates of the values of the independent components, and the sigmoid nonlinearities are obtained from the probability densities of the independent components.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Mendeley/Bingham - 2003 - ADVANCES IN INDEPENDENT COMPONENT ANALYSIS WITH APPLICATIONS TO DATA MINING.pdf},
  pages={211-222}
}
@article{Bouchaud_Financial_2009,
  year={2009},
  author={Bouchaud, J and Potters, M},
  title={Financial Applications of Random Matrix Theory: a short review},
  abstract={We discuss the applications of Random Matrix Theory in the context of financial markets and econometric models, a topic about which a considerable number of papers have been devoted to in the last decade. This mini-review is intended to guide the reader through various theoretical results (the {Marcenko-Pastur} spectrum and its various generalisations, random {SVD,} free matrices, largest eigenvalue statistics, etc.) as well as some concrete applications to portfolio optimisation and out-of-sample risk estimation.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Mendeley/Bouchaud, Potters - 1967 - Financial Applications of Random Matrix Theory a short review.pdf}
}
@article{Brandt_Parametric_2009,
  year={2009},
  journal={Rev Financ Stud},
  author={Brandt, Michael and {Santa-Clara,} Pedro and Valkanov, Rossen},
  volume={22},
  doi={10.1093/rfs/hhp003},
  title={Parametric Portfolio Policies: Exploiting Characteristics in the {Cross-Section} of Equity Returns},
  number={9},
  issn={0893-9454},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Mendeley/Brandt, Santa-clara - 2009 - Parametric Portfolio Policies Exploiting Characteristics in the Cross-Section of Equity Returns.pdf},
  pages={3411-3447}
}
@article{Brown_Dynamic_2011,
  year={2011},
  journal={Manage Sci},
  author={Brown, David and Smith, James},
  volume={57},
  doi={10.1287/mnsc.1110.1377},
  title={Dynamic Portfolio Optimization with Transaction Costs: Heuristics and Dual Bounds},
  number={10},
  issn={0025-1909},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Mendeley/Brown, Smith - 2011 - Dynamic Portfolio Optimization with Transaction Costs Heuristics and Dual Bounds.pdf},
  pages={1752-1770}
}
@article{Statistics_Convergence_2015,
  year={2015},
  journal={Nonlinear Analysis Model Control},
  author={Statistics, Xidian and Li, Jun and He, Chao and Statistics, Xidian},
  volume={20},
  doi={10.15388/NA.2015.4.1},
  title={Convergence analysis of estimated parameters for parametric nonlinear strict feedback system with unknown control direction},
  number={4},
  issn={1392-5113},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Mendeley/Date - Unknown - Kalman Filtering in Mathematical Finance.pdf},
  pages={469-486}
}
@article{Dean_MapReduce_2008,
  year={2008},
  journal={Commun Acm},
  author={Dean, Jeffrey and Ghemawat, Sanjay},
  volume={51},
  doi={10.1145/1327452.1327492},
  title={{MapReduce:} simplified data processing on large clusters},
  number={1},
  issn={0001-0782},
  abstract={{MapReduce} is a programming model and an associated implementation for processing and generating large datasets that is amenable to a broad variety of real-world tasks. Users specify the computation in terms of a map and a reduce function, and the underlying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make efficient use of the network and disks. Programmers find the system easy to use: more than ten thousand distinct {MapReduce} programs have been implemented internally at Google over the past four years, and an average of one hundred thousand {MapReduce} jobs are executed on Google's clusters every day, processing a total of more than twenty petabytes of data per day.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Mendeley/Dean, Ghemawat - 2008 - MapReduce Simplified Data Processing on Large Clusters.pdf},
  pages={107-113}
}
@article{Deistler_Identification_2005,
  year={2005},
  journal={J Financ Economet},
  author={Deistler, M},
  volume={3},
  doi={10.1093/jjfinec/nbi011},
  title={Identification of Factor Models for Forecasting Returns},
  number={2},
  issn={1479-8409},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Mendeley/Deistler - 2005 - Identification of Factor Models for Forecasting Returns.pdf},
  pages={256-281}
}
@book{Dobra_SECRET_2002,
  year={2002},
  abstract={Developing regression models for large datasets that are both accurate and easy to interpret is a very important data mining problem. Regression trees with linear models in the leaves satisfy both these requirements, but thus far, no truly scalable regression tree algorithm is known. This paper proposes a novel regression tree construction algorithm {(SECRET)} that produces trees of high quality and scales to very large datasets. At every node, {SECRET} uses the {EM} algorithm for Gaussian mixtures to find two clusters in the data and to locally transform the regression problem into a classification problem based on closeness to these clusters. Goodness of split measures, like the gini gain, can then be used to determine the split variable and the split point much like in classification tree construction. Scalability of the algorithm can be achieved by employing scalable versions of the {EM} and classification tree construction algorithms. An experimental evaluation on real and artificial data shows that {SECRET} has accuracy comparable to other linear regression tree algorithms but takes orders of magnitude less computation time for large datasets.},
  doi={10.1145/775047.775117},
  isbn={{978158113567X}},
  author={Dobra, Alin and Gehrke, Johannes},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Mendeley/Dobra, Gehrke - Unknown - SECRET A Scalable Linear Regression Tree Algorithm.pdf}
}
@article{Edelman_Random_2005,
  year={2005},
  journal={Acta Numer},
  author={Edelman, Alan and Rao, N},
  volume={14},
  doi={10.1017/S0962492904000236},
  title={Random matrix theory},
  issn={0962-4929},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Mendeley/Edelman - 2005 - Random matrix theory.pdf},
  pages={233-297}
}
@article{Elliott_Pairs_2005,
  year={2005},
  journal={Quant Financ},
  author={Elliott, Robert and *, John and Malcolm, William},
  volume={5},
  doi={10.1080/14697680500149370},
  title={Pairs trading},
  number={3},
  issn={1469-7688},
  abstract={Pairs Trading’ is an investment strategy used by many Hedge Funds. Consider two similar stocks which trade at some spread. If the spread widens short the high stock and buy the low stock. As the spread narrows again to some equilibrium value, a profit results. This paper provides an analytical framework for such an investment strategy. We propose a mean-reverting Gaussian Markov chain model for the spread which is observed in Gaussian noise. Predictions from the calibrated model are then compared with subsequent observations of the spread to determine appropriate investment decisions. The methodology has potential applications to generating wealth from any quantities in financial markets which are observed to be out of equilibrium.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Mendeley/Elliott, Van Der Hoek , Malcolm - 2005 - Pairs trading.pdf},
  pages={271-276}
}
@article{Evans_The_2008,
  year={2008},
  journal={Rev Netw Econ},
  author={Evans, David},
  volume={7},
  doi={10.2202/1446-9022.1154},
  title={The Economics of the Online Advertising Industry},
  number={3},
  issn={2194-5993},
  abstract={Internet-based technologies are revolutionizing the stodgy \$625 billion global advertising industry. There are a number of public policy issues to consider. Will a single ad platform emerge or will several remain viable? What are the consequences of alternative market structures for a web economy that is increasingly based on selling eyeballs to advertisers? This article describes the online advertising industry. The industry is populated by a number of multi-sided platforms that facilitate connecting advertisers to viewers. Search-based advertising platforms, the most developed of these, have interesting economic features that result from the combination of keyword bidding by advertisers and single-homing.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Mendeley/Evans - 2008 - THE ECONOMICS OF THE ONLINE ADVERTISING INDUSTRY.pdf}
}
@book{Broder_A_2007,
  year={2007},
  abstract={Contextual advertising or Context Match {(CM)} refers to the placement of commercial textual advertisements within the content of a generic web page, while Sponsored Search {(SS)} advertising consists in placing ads on result pages from a web search engine, with ads driven by the originating query. In {CM} there is usually an intermediary commercial ad-network entity in charge of optimizing the ad selection with the twin goal of increasing revenue (shared between the publisher and the ad-network) and improving the user experience. With these goals in mind it is preferable to have ads relevant to the page content, rather than generic ads. The {SS} market developed quicker than the {CM} market, and most textual ads are still characterized by "bid phrases" representing those queries where the advertisers would like to have their ad displayed. Hence, the first technologies for {CM} have relied on previous solutions for {SS,} by simply extracting one or more phrases from the given page content, and displaying ads corresponding to searches on these phrases, in a purely syntactic approach. However, due to the vagaries of phrase extraction, and the lack of context, this approach leads to many irrelevant ads. To overcome this problem, we propose a system for contextual ad matching based on a combination of semantic and syntactic features.},
  doi={10.1145/1277741.1277837},
  isbn={9781595935977},
  author={Broder, Andrei and Fontoura, Marcus and Josifovski, Vanja and Riedel, Lance},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Mendeley/Gabrilovich - 2006 - Towards better contextual advertising Background Online Advertising Robust Classification of Rare Queries Using We.pdf}
}
@book{Josifovski_Encyclopedia_2016,
  year={2016},
  doi={10.1007/978-1-4899-7993-3_455-2},
  title={Web Advertising},
  isbn={9781489979933},
  author={Josifovski, Vanja and Broder, Andrei},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Mendeley/Gabrilovich, Josifovski, Pang - 2008 - Introduction to computational advertising.pdf}
}
@article{GRLEANU_Dynamic_2013,
  year={2013},
  journal={J Finance},
  author={{G{\^A}RLEANU,} {NICOLAE} and {PEDERSEN,} {LASSE}},
  volume={68},
  doi={10.1111/jofi.12080},
  title={Dynamic Trading with Predictable Returns and Transaction Costs},
  number={6},
  issn={0022-1082},
  abstract={We derive a closed-form optimal dynamic portfolio policy when trading is costly and security returns are predictable by signals with different mean-reversion speeds. The optimal strategy is characterized by two principles: (1) aim in front of the target, and (2) trade partially toward the current aim. Specifically, the optimal updated portfolio is a linear combination of the existing portfolio and an “aim portfolio,” which is a weighted average of the current Markowitz portfolio (the moving target) and the expected Markowitz portfolios on all future dates (where the target is moving). Intuitively, predictors with slower mean-reversion (alpha decay) get more weight in the aim portfolio. We implement the optimal strategy for commodity futures and find superior net returns relative to more naive benchmarks.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Mendeley/Garleanu, Pedersen - Unknown - Dynamic Trading with Predictable Returns and Transaction Costs Dynamic Trading with Predictable Returns a.pdf},
  pages={2309-2340}
}
@article{Gomber_High_2013,
  year={2013},
  journal={Bus Information Syst Eng},
  author={Gomber, Peter and Haferkorn, Martin},
  volume={5},
  doi={10.1007/s12599-013-0255-7},
  title={{High-Frequency-Trading}},
  number={2},
  issn={1867-0202},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Mendeley/Gomber et al. - 2011 - High-Frequency Trading.pdf},
  pages={97-99}
}
@article{Gramacy_Shrinkage_2010,
  year={2010},
  journal={Bayesian Anal},
  author={Gramacy, Robert and Pantaleo, Ester},
  volume={5},
  doi={10.1214/10-BA602},
  title={Shrinkage regression for multivariate inference with missing data, and an application to portfolio balancing},
  number={2},
  issn={1936-0975},
  abstract={Portfolio balancing requires estimates of covariance between asset returns. Returns data have histories which greatly vary in length, since assets begin public trading at different times. This can lead to a huge amount of missing data--too much for the conventional imputation-based approach. Fortunately, a well-known factorization of the {MVN} likelihood under the prevailing historical missingness pattern leads to a simple algorithm of {OLS} regressions that is much more reliable. When there are more assets than returns, however, {OLS} becomes unstable. Gramacy, et al. (2008), showed how classical shrinkage regression may be used instead, thus extending the state of the art to much bigger asset collections, with further accuracy and interpretation advantages. In this paper, we detail a fully Bayesian hierarchical formulation that extends the framework further by allowing for heavy-tailed errors, relaxing the historical missingness assumption, and accounting for estimation risk. We illustrate how this approach compares favorably to the classical one using synthetic data and an investment exercise with real returns. An accompanying R package is on {CRAN.}},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Mendeley/Gramacy et al. - 2011 - Shrinkage regression for multivariate inference with missing data , and an application to portfolio balancing.pdf},
  pages={237-262}
}
@article{Hasanhodzic_A_2009,
  year={2009},
  author={Hasanhodzic, Jasmina and Lo, Andrew and Viola, Emanuele},
  title={A Computational View of Market Efficiency},
  abstract={We propose to study market efficiency from a computational viewpoint. Borrowing from theoretical computer science, we define a market to be \{\textbackslash\}emph\{efficient with respect to resources {\$S\$\}} (e.g., time, memory) if no strategy using resources {\$S\$} can make a profit. As a first step, we consider memory-\$m\$ strategies whose action at time \$t\$ depends only on the \$m\$ previous observations at times \$t-m,...,t-1\$. We introduce and study a simple model of market evolution, where strategies impact the market by their decision to buy or sell. We show that the effect of optimal strategies using memory \$m\$ can lead to "market conditions" that were not present initially, such as (1) market bubbles and (2) the possibility for a strategy using memory \$m' \{\textgreater\} m\$ to make a bigger profit than was initially possible. We suggest ours as a framework to rationalize the technological arms race of quantitative trading firms.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Mendeley/Hasanhodzic, Lo, Viola - 2009 - A Computational View of Market Efficiency.pdf}
}
@book{Granger_Aristotle_1996,
  year={1996},
  doi={10.1007/978-94-017-0785-5_1},
  title={Introduction},
  isbn={9789048147007},
  author={Granger, Herbert and Granger, Herbert},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Mendeley/Herlemont, Alexander - Unknown - Pairs Trading , Convergence Trading , Cointegration.pdf}
}
@article{White_A_2000,
  year={2000},
  journal={Econometrica},
  author={White, Halbert},
  volume={68},
  doi={10.1111/1468-0262.00152},
  title={A Reality Check for Data Snooping},
  number={5},
  issn={1468-0262},
  abstract={Data snooping occurs when a given set of data is used more than once for purposes of inference or model selection. When such data reuse occurs, there is always the possibility that any satisfactory results obtained may simply be due to chance rather than to any merit inherent in the method yielding the results. This problem is practically unavoidable in the analysis of time-series data, as typically only a single history measuring a given phenomenon of interest is available for analysis. It is widely acknowledged by empirical researchers that data snooping is a dangerous practice to be avoided, but in fact it is endemic. The main problem has been a lack of sufficiently simple practical methods capable of assessing the potential dangers of data snooping in a given situation. Our purpose here is to provide such methods by specifying a straightforward procedure for testing the null hypothesis that the best model encountered in a specification search has no predictive superiority over a given benchmark model. This permits data snooping to be undertaken with some degree of confidence that one will not mistake results that could have been generated by chance for genuinely good results.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Mendeley/Hite - 2000 - A reality check for data snooping b.pdf},
  pages={1097-1126}
}
@book{Teh_Bayesian_2010,
  year={2010},
  abstract={Introduction Hierarchical modeling is a fundamental concept in Bayesian statistics. The basic idea is that parameters are endowed with distributions which may themselves introduce new parameters, and this construction recurses. A common motif in hierarchical modeling is that of the conditionally independent hierarchy, in which a set of parameters are coupled by making their distributions depend on a shared underlying parameter. These distributions are often taken to be identical, based on an assertion of exchangeability and an appeal to de Finetti's theorem.},
  doi={10.1017/CBO9780511802478.006},
  title={Hierarchical Bayesian nonparametric models with applications},
  isbn={9780511802478},
  author={Teh, Yee and Jordan, Michael},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Mendeley/Jordan - 1980 - Bayesian Nonparametric Learning Expres- sive Priors for Intelligent Systems.pdf}
}
@article{Keshavan_Matrix_2009,
  year={2009},
  author={Keshavan, Raghunandan and Montanari, Andrea and Oh, Sewoong},
  title={Matrix Completion from Noisy Entries},
  abstract={Given a matrix M of low-rank, we consider the problem of reconstructing it from noisy observations of a small, random subset of its entries. The problem arises in a variety of applications, from collaborative filtering (the {‘Netflix} problem') to structure-from-motion and positioning. We study a low complexity algorithm introduced by Keshavan et al.(2009), based on a combination of spectral techniques and manifold optimization, that we call here {OptSpace.} We prove performance guarantees that are order-optimal in a number of circumstances.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Mendeley/Keshavan - 2009 - Matrix Completion from Noisy Entries(2).pdf}
}
@article{Keshavan_Matrix_2009,
  year={2009},
  author={Keshavan, Raghunandan and Montanari, Andrea and Oh, Sewoong},
  title={Matrix Completion from a Few Entries},
  abstract={Let M be a random (alpha n) x n matrix of rank r\{\textless\}\{\textless\}n, and assume that a uniformly random subset E of its entries is observed. We describe an efficient algorithm that reconstructs M from {\{\textbar\}E\{\textbar\}} = O(rn) observed entries with relative root mean square error {RMSE} \{\textless\}= {C(rn/\{\textbar\}E\{\textbar\})\{\textasciicircum\}0.5} . Further, if {r=O(1),} M can be reconstructed exactly from {\{\textbar\}E\{\textbar\}} = O(n log(n)) entries. These results apply beyond random matrices to general low-rank incoherent matrices. This settles (in the case of bounded rank) a question left open by Candes and Recht and improves over the guarantees for their reconstruction algorithm. The complexity of our algorithm is {O(\{\textbar\}E\{\textbar\}r} log(n)), which opens the way to its use for massive data sets. In the process of proving these statements, we obtain a generalization of a celebrated result by {Friedman-Kahn-Szemeredi} and {Feige-Ofek} on the spectrum of sparse random matrices.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Mendeley/Keshavan - 2009 - Matrix Completion from Noisy Entries.pdf}
}
@article{Laloux_Noise_1999,
  year={1999},
  journal={Phys Rev Lett},
  author={Laloux, Laurent and Cizeau, Pierre and Bouchaud, {Jean-Philippe} and Potters, Marc},
  volume={83},
  doi={10.1103/PhysRevLett.83.1467},
  title={Noise Dressing of Financial Correlation Matrices},
  number={7},
  issn={1079-7114},
  abstract={We show that results from the theory of random matrices are potentially of great interest to understand the statistical structure of the empirical correlation matrices appearing in the study of multivariate time series. The central result of the present study, which focuses on the case of financial price fluctuations, is the remarkable agreement between the theoretical prediction (based on the assumption that the correlation matrix is random) and empirical data concerning the density of eigenvalues associated to the time series of the different stocks of the {S\&P} 500 (or other major markets). In particular, the present study raises serious doubts on the blind use of empirical correlation matrices for risk management.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Mendeley/Laloux et al. - 2008 - Noise Dressing of Financial Correlation Matrices.pdf},
  pages={1467}
}
@article{Jacobs_Portfolio_2005,
  year={2005},
  journal={Oper Res},
  author={Jacobs, Bruce and Levy, Kenneth and Markowitz, Harry},
  volume={53},
  doi={10.1287/opre.1050.0212},
  title={Portfolio Optimization with Factors, Scenarios, and Realistic Short Positions},
  number={4},
  issn={{0030-364X}},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Mendeley/Levy et al. - 2005 - Portfolio Optimization with Factors , Scenarios , and.pdf},
  pages={586-599}
}
@article{MacLean_Capital_2004,
  year={2004},
  journal={J Econ Dyn Control},
  author={{MacLean,} Leonard and Sanegre, Rafael and Zhao, Yonggan and Ziemba, William},
  volume={28},
  doi={10.1016/S0165-1889(03)00056-3},
  title={Capital growth with security},
  number={5},
  issn={0165-1889},
  abstract={This paper discusses the allocation of capital over time with several risky assets. The capital growth log utility approach is used with conditions requiring that specific goals are achieved with high probability. The stochastic optimization model uses a disjunctive form for the probabilistic constraints, which identifies an outer problem of choosing an optimal set of scenarios, and an inner (conditional) problem of finding the optimal investment decisions for a given scenarios set. The multiperiod inner problem is composed of a sequence of conditional one period problems. The theory is illustrated for the dynamic allocation of wealth in stocks, bonds and cash equivalents.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Mendeley/Maclean et al. - 2004 - Capital growth with security.pdf},
  pages={937-954}
}
@article{Feldman_Algorithmic_2008,
  year={2008},
  author={Feldman, Jon and Muthukrishnan, S},
  title={Algorithmic Methods for Sponsored Search Advertising},
  abstract={Modern commercial Internet search engines display advertisements along side the search results in response to user queries. Such sponsored search relies on market mechanisms to elicit prices for these advertisements, making use of an auction among advertisers who bid in order to have their ads shown for specific keywords. We present an overview of the current systems for such auctions and also describe the underlying game-theoretic aspects. The game involves three parties--advertisers, the search engine, and search users--and we present example research directions that emphasize the role of each. The algorithms for bidding and pricing in these games use techniques from three mathematical areas: mechanism design, optimization, and statistical estimation. Finally, we present some challenges in sponsored search advertising.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Mendeley/May - Unknown - Advertising.pdf}
}
@article{Michaud_Efficient_2001,
  year={2001},
  journal={Rev Financ Stud},
  author={Michaud, Richard and Ma, Tongshu},
  volume={14},
  doi={10.1093/rfs/14.3.901},
  title={Efficient Asset Management: A Practical Guide to Stock Portfolio Optimization and Asset Allocation.},
  number={3},
  issn={0893-9454},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Mendeley/Michaud, Michaud - 2004 - Forecast Confidence Level and Portfolio Optimization by.pdf},
  pages={901-904}
}
@article{Laboratory_Follicular_2015,
  year={2015},
  journal={Int J Vet Heal Sci Res},
  author={Laboratory, University and K, Singh and Z, Demeter and Inc, 2825 and P, Kumar and Medicine, Kansas},
  doi={10.19070/2332-2748-150004e},
  title={Follicular Lymphoma in Companion Animals},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Mendeley/Mungamuru, Weis - Unknown - Competition and Fraud in Online Advertising Markets.pdf},
  pages={1}
}
@book{Muthu_Empire_2012,
  year={2012},
  abstract={European political thought from the Renaissance through the nineteenth century has long been associated with theorizations of a few key social, political, and economic developments: the rise of “the state” and its primary subinstitutions and practices, such as standing armies, central banks, and bureaucratic administrations; the reordering of relationships among religious institutions and political powers, at times leading to governments’ greater or lesser toleration of diverse religious practices and denominations; the development of commercial systems of trade, mass manufacturing (eventually industrial production), chartered companies with transnational operations, and related debates about consumption, luxury, and the social and political effects of growing merchant classes; and the ideologies of natural (or human) rights and of republican (or democratic) forms of governance. Many of the political events that are taken to be constitutive of this period, from the Thirty Years War to the Edict of Nantes (and its revocation) to the upheavals of 1688, 1776, and 1789 were both influenced by and shaped modern political discourses. When one adds to these developments the significant impact of the rise of modern science, including the experimental sciences, on moral and political writings as well as the technological breakthroughs that made accurate oceanic navigation and industrial manufacturing possible, the profound transformations in social thought in this period cannot be underestimated. The global and imperial dimensions of this period, however, and in particular the self-conscious theorization of them in past centuries, have been relatively neglected by historians of political thought when compared to the vast amount of scholarly work in political theory about, for example, modern political philosophies of revolution, toleration, and the state. While historians of modern political thought have occasionally turned to the global dimensions of this period, only in the past roughly dozen years has a critical mass of such scholars analyzed the importance of territorial expansion and transcontinental (often imperial) networks of direct or indirect imperial governance, naval and military activity, and trade in the writings of modern European political thinkers.},
  doi={10.1017/CBO9781139016285.001},
  title={Introduction},
  isbn={9781139016285},
  author={Muthu, Sankar},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Mendeley/Muthukrishnan - 2008 - Internet Ad Auctions Insights and Directions.pdf}
}
@article{Peters_Optimal_2009,
  year={2009},
  author={Peters, Ole},
  doi={10.1080/14697688.2010.513338},
  title={Optimal leverage from non-ergodicity},
  abstract={In modern portfolio theory, the balancing of expected returns on investments against uncertainties in those returns is aided by the use of utility functions. The Kelly criterion offers another approach, rooted in information theory, that always implies logarithmic utility. The two approaches seem incompatible, too loosely or too tightly constraining investors' risk preferences, from their respective perspectives. The conflict can be understood on the basis that the multiplicative models used in both approaches are non-ergodic which leads to ensemble-average returns differing from time-average returns in single realizations. The classic treatments, from the very beginning of probability theory, use ensemble-averages, whereas the Kelly-result is obtained by considering time-averages. Maximizing the time-average growth rates for an investment defines an optimal leverage, whereas growth rates derived from ensemble-average returns depend linearly on leverage. The latter measure can thus incentivize investors to maximize leverage, which is detrimental to time-average growth and overall market stability. The Sharpe ratio is insensitive to leverage. Its relation to optimal leverage is discussed. A better understanding of the significance of time-irreversibility and non-ergodicity and the resulting bounds on leverage may help policy makers in reshaping financial risk controls.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Mendeley/Peters - 2009 - Optimal leverage from non-ergodicity.pdf}
}
@article{Plerou_Random_2002,
  year={2002},
  author={Plerou, Vasiliki and Gopikrishnan, Parameswaran and Rosenow, Bernd and Amaral, Lu{\'i}s A and Guhr, Thomas and Stanley, Eugene H},
  volume={65},
  doi={10.1103/PhysRevE.65.066126},
  title={Random matrix approach to cross correlations in financial data},
  number={6},
  pmid={12188802},
  issn={2470-0053},
  abstract={We analyze cross correlations between price fluctuations of different stocks using methods of random matrix theory {(RMT).} Using two large databases, we calculate cross-correlation matrices C of returns constructed from (i) 30-min returns of 1000 {US} stocks for the 2-yr period 1994–1995, (ii) 30-min returns of 881 {US} stocks for the 2-yr period 1996–1997, and (iii) 1-day returns of 422 {US} stocks for the 35-yr period 1962–1996. We test the statistics of the eigenvalues λi of C against a “null hypothesis” — a random correlation matrix constructed from mutually uncorrelated time series. We find that a majority of the eigenvalues of C fall within the {RMT} bounds [λ−,λ+] for the eigenvalues of random correlation matrices. We test the eigenvalues of C within the {RMT} bound for universal properties of random matrices and find good agreement with the results for the Gaussian orthogonal ensemble of random matrices—implying a large degree of randomness in the measured cross-correlation coefficients. Further, we find that the distribution of eigenvector components for the eigenvectors corresponding to the eigenvalues outside the {RMT} bound display systematic deviations from the {RMT} prediction. In addition, we find that these “deviating eigenvectors” are stable in time. We analyze the components of the deviating eigenvectors and find that the largest eigenvalue corresponds to an influence common to all stocks. Our analysis of the remaining deviating eigenvectors shows distinct groups, whose identities correspond to conventionally identified business sectors. Finally, we discuss applications to the construction of portfolios of stocks that have a stable ratio of risk to return.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Mendeley/Plerou et al. - 2001 - A Random Matrix Approach to Cross-Correlations in Financial Data.pdf;/Users/pchalasani/Dropbox/MediaMath/Papers/Mendeley/Amaral et al. - 2002 - Random matrix approach to cross correlations in financial data.pdf},
  pages={066126}
}
@article{Jacobs_Trimability_2006,
  year={2006},
  journal={Financ Anal J},
  author={Jacobs, Bruce and Levy, Kenneth and Markowitz, Harry},
  volume={62},
  doi={10.2469/faj.v62.n2.4082},
  title={Trimability and Fast Optimization of {Long–Short} Portfolios},
  number={2},
  issn={{0015-198X}},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Mendeley/Portfolios et al. - 2006 - Trimability and Fast Optimization of.pdf},
  pages={36-46}
}
@article{Rutz_Zooming_2011,
  year={2011},
  journal={Market Sci},
  author={Rutz, Oliver and Trusov, Michael},
  volume={30},
  doi={10.1287/mksc.1110.0647},
  title={Zooming In on Paid Search {Ads—A} {Consumer-Level} Model Calibrated on Aggregated Data},
  number={5},
  issn={0732-2399},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Mendeley/Rutz, Trusov - 2010 - Zooming In on Paid Search Ads – A Consumer-level Model Calibrated on Aggregated Data.pdf},
  pages={789-800}
}
@article{Shlens_A_2014,
  year={2014},
  author={Shlens, Jonathon},
  title={A Tutorial on Principal Component Analysis},
  abstract={Principal component analysis {(PCA)} is a mainstay of modern data analysis - a black box that is widely used but (sometimes) poorly understood. The goal of this paper is to dispel the magic behind this black box. This manuscript focuses on building a solid intuition for how and why principal component analysis works. This manuscript crystallizes this knowledge by deriving from simple intuitions, the mathematics behind {PCA.} This tutorial does not shy away from explaining the ideas informally, nor does it shy away from the mathematics. The hope is that by addressing both aspects, readers of all levels will be able to gain a better understanding of {PCA} as well as the when, the how and the why of applying this technique.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Mendeley/Shlens - 2009 - A Tutorial on Principal Component Analysis.pdf}
}
@book{Shrivastava_Time_2016,
  year={2016},
  abstract={Obtaining frequency information of data streams, in limited space, is a well-recognized problem in literature. A number of recent practical applications (such as those in computational advertising) require temporally-aware solutions: obtaining historical count statistics for both time-points as well as time-ranges. In these scenarios, accuracy of estimates is typically more important for recent instances than for older ones; we call this desirable property Time Adaptiveness. With this observation, [20] introduced the Hokusai technique based on count-min sketches for estimating the frequency of any given item at any given time. The proposed approach is problematic in practice, as its memory requirements grow linearly with time, and it produces discontinuities in the estimation accuracy. In this work, we describe a new method, Time-adaptive Sketches, {(Ada-sketch),} that overcomes these limitations, while extending and providing a strict generalization of several popular sketching algorithms. The core idea of our method is inspired by the well-known digital Dolby noise reduction procedure that dates back to the 1960s. The theoretical analysis presented could be of independent interest in itself, as it provides clear results for the time-adaptive nature of the errors. An experimental evaluation on real streaming datasets demonstrates the superiority of the described method over Hokusai in estimating point and range queries over time. The method is simple to implement and offers a variety of design choices for future extensions. The simplicity of the procedure and the method's generalization of classic sketching techniques give hope for wide applicability of Ada-sketches in practice.},
  doi={10.1145/2882903.2882946},
  isbn={9781450335317},
  author={Shrivastava, Anshumali and Konig, Arnd and Bilenko, Mikhail},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Mendeley/Shrivastava, König, Bilenko - Unknown - Time Adaptive Sketches (Ada-Sketches) for Summarizing Data Streams.pdf}
}
@article{Scott_A_2005,
  year={2005},
  journal={Ieee T Inform Theory},
  author={Scott, C and Nowak, R},
  volume={51},
  doi={10.1109/TIT.2005.856955},
  title={A {Neyman–Pearson} Approach to Statistical Learning},
  number={11},
  issn={0018-9448},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Mendeley/Speybroeck - 2012 - Classification and regression trees.pdf},
  pages={3806-3819}
}
@article{Triantafyllopoulos_Dynamic_2008,
  year={2008},
  author={Triantafyllopoulos, Kostas and Montana, Giovanni},
  title={Dynamic modeling of mean-reverting spreads for statistical arbitrage},
  abstract={Statistical arbitrage strategies, such as pairs trading and its generalizations, rely on the construction of mean-reverting spreads enjoying a certain degree of predictability. Gaussian linear state-space processes have recently been proposed as a model for such spreads under the assumption that the observed process is a noisy realization of some hidden states. Real-time estimation of the unobserved spread process can reveal temporary market inefficiencies which can then be exploited to generate excess returns. Building on previous work, we embrace the state-space framework for modeling spread processes and extend this methodology along three different directions. First, we introduce time-dependency in the model parameters, which allows for quick adaptation to changes in the data generating process. Second, we provide an on-line estimation algorithm that can be constantly run in real-time. Being computationally fast, the algorithm is particularly suitable for building aggressive trading strategies based on high-frequency data and may be used as a monitoring device for mean-reversion. Finally, our framework naturally provides informative uncertainty measures of all the estimated parameters. Experimental results based on Monte Carlo simulations and historical equity data are discussed, including a co-integration relationship involving two exchange-traded funds.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Mendeley/Triantafyllopoulos - 2009 - Dynamic modeling of mean-reverting spreads for statistical arbitrage.pdf}
}
@article{Tripathy_SDR_2011,
  year={2011},
  author={Tripathy, B and Ghosh, Adhir},
  doi={10.1109/RAICS.2011.6069433},
  title={{SDR:} An algorithm for clustering categorical data using rough set theory},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Mendeley/Tripathy, Ghosh - 2011 - SSDR An Algorithm for Clustering Categorical Data Using Rough Set Theory.pdf},
  pages={867-872}
}
@article{Jacobs_Long_1999,
  year={1999},
  journal={J Portfolio Management},
  author={Jacobs, Bruce and Levy, Kenneth and Starer, David},
  volume={25},
  doi={10.3905/jpm.1999.319730},
  title={{Long-Short} Portfolio Management},
  number={2},
  issn={0095-4918},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Mendeley/Waterman, Banerjee - 2011 - Management of simultaneous ipsilateral dislocation of hip, knee, and ankle.pdf},
  pages={23-32}
}
@article{Medo_How_2008,
  year={2008},
  author={Medo, Matus and Yeung, Chi and Zhang, {Yi-Cheng}},
  doi={10.1016/j.irfa.2009.01.001},
  title={How to quantify the influence of correlations on investment diversification},
  abstract={When assets are correlated, benefits of investment diversification are reduced. To measure the influence of correlations on investment performance, a new quantity - the effective portfolio size - is proposed and investigated in both artificial and real situations. We show that in most cases, the effective portfolio size is much smaller than the actual number of assets in the portfolio and that it lowers even further during financial crises.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Mendeley/Yeung, Zhang - Unknown - How to quantify the influence of correlations on investment diversification.pdf}
}
@article{Medo_Diversification_2008,
  year={2008},
  author={Medo, Mat{\'u}š and Pis’mak, Yury M and Zhang, {Yi-Cheng}},
  volume={387},
  doi={10.1016/j.physa.2008.07.007},
  title={Diversification and limited information in the Kelly game},
  number={24},
  issn={0378-4371},
  abstract={Financial markets, with their vast range of different investment opportunities, can be seen as a system of many different simultaneous games with diverse and often unknown levels of risk and reward. We introduce generalizations to the classic Kelly investment game {[J.L.} Kelly, {IEEE} Transactions on Information Theory 2 (1956) 185–189] that incorporates these features, and use them to investigate the influence of diversification and limited information on Kelly-optimal portfolios. In particular, we present approximate formulas for optimizing diversified portfolios and exact results for optimal investment in unknown games where the only available information is past outcomes.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Mendeley/Zhang - 1956 - Diversification and limited information in the Kelly game.pdf;/Users/pchalasani/Dropbox/MediaMath/Papers/Mendeley/Medo, Pis, Zhang - 2008 - Author ' s personal copy Diversification and limited information in the Kelly game Author ' s personal copy.pdf},
  pages={6151-6158}
}
@article{Zio_Imputation_2007,
  year={2007},
  journal={Comput Statistics Data Analysis},
  author={Zio, Marco and Guarnera, Ugo and Luzi, Orietta},
  volume={51},
  doi={10.1016/j.csda.2006.10.002},
  title={Imputation through finite Gaussian mixture models},
  number={11},
  issn={0167-9473},
  abstract={Imputation is a widely used method for handling missing data. It consists in the replacement of missing values with plausible ones. Parametric and nonparametric techniques are generally adopted for modelling incomplete data. Both of them have advantages and drawbacks. Parametric techniques are parsimonious but depend on the model assumed, while nonparametric techniques are more flexible but require a high amount of observations. The use of finite mixture of multivariate Gaussian distributions for handling missing data is proposed. The main reason is that it allows to control the trade-off between parsimony and flexibility. An experimental comparison with the widely used imputation nearest neighbour donor is illustrated.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Mendeley/Zio, Guarnera, Luzi - 2007 - Imputation through finite Gaussian mixture models.pdf},
  pages={5305-5316}
}
@article{Zhang_Implicit_2016,
  year={2016},
  author={Zhang, Weinan and Chen, Lingxi and Wang, Jun},
  title={Implicit Look-alike Modelling in Display Ads: Transfer Collaborative Filtering to {CTR} Estimation},
  abstract={User behaviour targeting is essential in online advertising. Compared with sponsored search keyword targeting and contextual advertising page content targeting, user behaviour targeting builds users' interest profiles via tracking their online behaviour and then delivers the relevant ads according to each user's interest, which leads to higher targeting accuracy and thus more improved advertising performance. The current user profiling methods include building keywords and topic tags or mapping users onto a hierarchical taxonomy. However, to our knowledge, there is no previous work that explicitly investigates the user online visits similarity and incorporates such similarity into their ad response prediction. In this work, we propose a general framework which learns the user profiles based on their online browsing behaviour, and transfers the learned knowledge onto prediction of their ad response. Technically, we propose a transfer learning model based on the probabilistic latent factor graphic models, where the users' ad response profiles are generated from their online browsing profiles. The large-scale experiments based on real-world data demonstrate significant improvement of our solution over some strong baselines.},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/Mendeley/Zhang, Chen, Wang - 2016 - Implicit Look-alike Modelling in Display Ads – Transfer Collaborative Filtering to CTR Estimation.pdf}
}
@article{Joulani_Online_2013,
  year={2013},
  journal={{ICML} (3)},
  author={Joulani, Pooria and Gy{\"o}rgy, Andr{\'a}s and Szepesv{\'a}ri, Csaba},
  title={Online Learning under Delayed Feedback.},
  abstract={Abstract Online learning with delayed feedback has received increasing attention recently due to its several applications in distributed, web-based learning problems. In this paper we provide a systematic study of the topic, and analyze the effect of delay on the regret of online },
  file={/Users/pchalasani/Documents/ReadCube Media/Joulani et al-2013 2.pdf;/Users/pchalasani/Documents/ReadCube Media/Joulani et al-2013 3.pdf},
  pages={1453-1461}
}
@article{Xu_Lift_2015,
  year={2015},
  journal={{arXiv} preprint {arXiv:1507.04811}},
  author={Xu, J and Shao, X and Ma, J and Lee, K and Qi, H and Lu, Q},
  title={Lift-based bidding in ad selection},
  abstract={Abstract: Real-time bidding {(RTB)} has become one of the largest online advertising markets in the world. Today the bid price per ad impression is typically decided by the expected value of how it can lead to a desired action event (eg, registering an account or placing a },
  file={/Users/pchalasani/Documents/ReadCube Media/Xu et al-2015-arXiv preprint arXiv150704811.pdf}
}
@article{4029acad-f07d-44a2-8680-609cd4cbf847,
  year={2015},
  author={Johnson, {GA} and Lewis, {RA}},
  title={poseidon01.ssrn.com 1/24/2017, 10:47:29 {PM.pdf}},
  abstract={Abstract: Advertisers seek to maximize profits by investing in advertising. We propose a {“cost-per-incremental-action”(CPIA)} pricing model which incorporates the causal contribution of advertising in order to achieve the advertisers' objectives such as profit maximization. {CPIA} pricing aligns marketplace incentives among all participants to help advertisers achieve their objectives via ad effectiveness and, by doing so, eliminates the adverse behaviors  ...}
}
@article{Gordon_A_2016,
  year={2016},
  author={Gordon, Brett and Zettelmeyer, Florian and Bhargava, Neha and Chapsky, Dan},
  title={A comparison of approaches to advertising measurement: Evidence from big field experiments at Facebook},
  abstract={Abstract We examine how common techniques used to measure the causal impact of ad exposures on users' conversion outcomes compare to the “gold standard” of a true experiment (randomized controlled trial). Using data from 12 {US} advertising lift studies at },
  file={/Users/pchalasani/Documents/ReadCube Media/Gordon et al-2016-White paper.pdf}
}
@book{Agarwal_Estimating_2010,
  year={2010},
  abstract={We consider the problem of estimating rates of rare events for high dimensional, multivariate categorical data where several dimensions are hierarchical. Such problems are routine in several data mining applications including computational advertising, our main focus in this paper. We propose {LMMH,} a novel log-linear modeling method that scales to massive data applications with billions of training records and several million potential predictors in a map-reduce framework. Our method exploits correlations in aggregates observed at multiple resolutions when working with multiple hierarchies; stable estimates at coarser resolution provide informative prior information to improve estimates at finer resolutions. Other than prediction accuracy and scalability, our method has an inbuilt variable screening procedure based on a "spike and slab prior" that provides parsimony by removing non-informative predictors without hurting predictive accuracy. We perform large scale experiments on data from real computational advertising applications and illustrate our approach on datasets with several billion records and hundreds of millions of predictors. Extensive comparisons with other benchmark methods show significant improvements in prediction accuracy.},
  doi={10.1145/1835804.1835834},
  isbn={9781450300551},
  author={Agarwal, Deepak and Agrawal, Rahul and Khanna, Rajiv and Kota, Nagaraj}
}
@article{Copas_Missing_2004,
  year={2004},
  journal={Ann Statistics},
  author={Copas, John B and Lu, Guobing},
  volume={32},
  doi={10.1214/009053604000000166},
  title={Missing at random, likelihood ignorability and model completeness},
  number={2},
  issn={0090-5364},
  abstract={This paper provides further insight into the key concept of missing at random {(MAR)} in incomplete data analysis. Following the usual selection modelling approach we envisage two models with separable parameters: a model for the response of interest and a model for the missing data mechanism {(MDM).} If the response model is given by a complete density family, then frequentist inference from the likelihood function ignoring the {MDM} is valid if and only if the {MDM} is {MAR.} This necessary and sufficient condition also holds more generally for models for coarse data, such as censoring. Examples are given to show the necessity of the completeness of the underlying model for this equivalence to hold.},
  pages={754-765}
}
@article{Vasile_Cost_2016,
  year={2016},
  author={Vasile, Flavian and Lefortier, Damien and Chapelle, Olivier},
  title={Cost-sensitive Learning for Utility Optimization in Online Advertising Auctions},
  abstract={One of the most challenging problems in computational advertising is the prediction of click-through and conversion rates for bidding in online advertising auctions. An unaddressed problem in previous approaches is the existence of highly non-uniform misprediction costs. While for model evaluation these costs have been taken into account through recently proposed business-aware offline metrics, such as the Utility metric which measures the impact on advertiser profit, this is not the case when training the models themselves. In this paper, to bridge the gap, we formally analyse the relationship between optimizing the Utility metric and the log loss, which is considered as one of the state-of-the-art approaches in conversion modelling. Our analysis motivates the idea of weighting the log loss with the business value of the predicted outcome. We present and analyse a new cost weighting scheme and show that significant gains in offline and online performance can be achieved.}
}
@book{Menon_Response_2011,
  year={2011},
  abstract={In online advertising, response prediction is the problem of estimating the probability that an advertisement is clicked when displayed on a content publisher's webpage. In this paper, we show how response prediction can be viewed as a problem of matrix completion, and propose to solve it using matrix factorization techniques from collaborative filtering {(CF).} We point out the two crucial differences between standard {CF} problems and response prediction, namely the requirement of predicting probabilities rather than scores, and the issue of confidence in matrix entries. We address these issues using a matrix factorization analogue of logistic regression, and by applying a principled confidence-weighting scheme to its objective. We show how this factorization can be seamlessly combined with explicit features or side-information for pages and ads, which let us combine the benefits of both approaches. Finally, we combat the extreme sparsity of response prediction data by incorporating hierarchical information about the pages and ads into our factorization model. Experiments on three very large real-world datasets show that our model outperforms current state-of-the-art methods for response prediction.},
  doi={10.1145/2020408.2020436},
  isbn={9781450308137},
  author={Menon, Aditya and Chitrapura, {Krishna-Prasad} and Garg, Sachin and Agarwal, Deepak and Kota, Nagaraj}
}
@article{Chapelle_Simple_2015,
  year={2015},
  author={Chapelle, Olivier and Manavoglu, Eren and Rosales, Romer},
  volume={5},
  title={Simple and scalable response prediction for display advertising}
}
@article{Tkachenko_Optimal_2014,
  year={2014},
  author={Tkachenko, Yegor},
  volume={2},
  doi={10.1057/jma.2014.14},
  title={Optimal allocation of digital marketing budget: The empirical Bayes approach},
  number={3},
  issn={2050-3318},
  abstract={The article outlines a framework for online advertising budget allocation. First, it explores the empirical Bayes methodology for learning the effectiveness of different online ad placements – from historical data of varying quality. Second, it describes an analytical procedure for optimal budget allocation, which builds on risk management and reinforcement learning techniques.},
  pages={162-172}
}
@article{Shariat_Online_2015,
  year={2015},
  author={Shariat, Shahriar and Orten, Burkay and Dasdan, Ali},
  title={Online Model Evaluation in a {Large-Scale} Computational Advertising Platform},
  abstract={Online media provides opportunities for marketers through which they can deliver effective brand messages to a wide range of audiences. Advertising technology platforms enable advertisers to reach their target audience by delivering ad impressions to online users in real time. In order to identify the best marketing message for a user and to purchase impressions at the right price, we rely heavily on bid prediction and optimization models. Even though the bid prediction models are well studied in the literature, the equally important subject of model evaluation is usually overlooked. Effective and reliable evaluation of an online bidding model is crucial for making faster model improvements as well as for utilizing the marketing budgets more efficiently. In this paper, we present an experimentation framework for bid prediction models where our focus is on the practical aspects of model evaluation. Specifically, we outline the unique challenges we encounter in our platform due to a variety of factors such as heterogeneous goal definitions, varying budget requirements across different campaigns, high seasonality and the auction-based environment for inventory purchasing. Then, we introduce return on investment {(ROI)} as a unified model performance (i.e., success) metric and explain its merits over more traditional metrics such as click-through rate {(CTR)} or conversion rate {(CVR).} Most importantly, we discuss commonly used evaluation and metric summarization approaches in detail and propose a more accurate method for online evaluation of new experimental models against the baseline. Our meta-analysis-based approach addresses various shortcomings of other methods and yields statistically robust conclusions that allow us to conclude experiments more quickly in a reliable manner. We demonstrate the effectiveness of our evaluation strategy on real campaign data through some experiments.}
}
@book{Liu_A_2015,
  year={2015},
  abstract={The explosion in online advertisement urges to better estimate the click prediction of ads. For click prediction on single ad impression, we have access to pairwise relevance among elements in an impression, but not to global interaction among key features of elements. Moreover, the existing method on sequential click prediction treats propagation unchangeable for different time intervals. In this work, we propose a novel model, Convolutional Click Prediction Model {(CCPM),} based on convolution neural network. {CCPM} can extract local-global key features from an input instance with varied elements, which can be implemented for not only single ad impression but also sequential ad impression. Experiment results on two public large-scale datasets indicate that {CCPM} is effective on click prediction.},
  doi={10.1145/2806416.2806603},
  isbn={9781450337946},
  author={Liu, Qiang and Yu, Feng and Wu, Shu and Wang, Liang},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/ML/ConvolutionalClickPrediction.pdf}
}
@book{Ren_User_2016,
  year={2016},
  abstract={Learning and predicting user responses, such as clicks and conversions, are crucial for many Internet-based businesses including web search, e-commerce, and online advertising. Typically, a user response model is established by optimizing the prediction accuracy, e.g., minimizing the error between the prediction and the ground truth user response. However, in many practical cases, predicting user responses is only part of a rather larger predictive or optimization task, where on one hand, the accuracy of a user response prediction determines the final (expected) utility to be optimized, but on the other hand, its learning may also be influenced from the follow-up stochastic process. It is, thus, of great interest to optimize the entire process as a whole rather than treat them independently or sequentially. In this paper, we take real-time display advertising as an example, where the predicted user's ad click-through rate {(CTR)} is employed to calculate a bid for an ad impression in the second price auction. We reformulate a common logistic regression {CTR} model by putting it back into its subsequent bidding context: rather than minimizing the prediction error, the model parameters are learned directly by optimizing campaign profit. The gradient update resulted from our formulations naturally fine-tunes the cases where the market competition is high, leading to a more cost-effective bidding. Our experiments demonstrate that, while maintaining comparable {CTR} prediction accuracy, our proposed user response learning leads to campaign profit gains as much as 78.2\% for offline test and 25.5\% for online {A/B} test over strong baselines.},
  doi={10.1145/2983323.2983347},
  isbn={9781450340731},
  author={Ren, Kan and Zhang, Weinan and Rong, Yifei and Zhang, Haifeng and Yu, Yong and Wang, Jun}
}
@article{Amin_Budget_2012,
  year={2012},
  author={Amin, Kareem and Kearns, Michael and Key, Peter and Schwaighofer, Anton},
  title={Budget Optimization for Sponsored Search: Censored Learning in {MDPs}},
  abstract={We consider the budget optimization problem faced by an advertiser participating in repeated sponsored search auctions, seeking to maximize the number of clicks attained under that budget. We cast the budget optimization problem as a Markov Decision Process {(MDP)} with censored observations, and propose a learning algorithm based on the wellknown {Kaplan-Meier} or product-limit estimator. We validate the performance of this algorithm by comparing it to several others on a large set of search auction data from Microsoft {adCenter,} demonstrating fast convergence to optimal performance.}
}
@article{Swaminathan_Counterfactual_2015,
  year={2015},
  author={Swaminathan, Adith and Joachims, Thorsten},
  title={Counterfactual Risk Minimization: Learning from Logged Bandit Feedback},
  abstract={We develop a learning principle and an efficient algorithm for batch learning from logged bandit feedback. This learning setting is ubiquitous in online systems (e.g., ad placement, web search, recommendation), where an algorithm makes a prediction (e.g., ad ranking) for a given input (e.g., query) and observes bandit feedback (e.g., user clicks on presented ads). We first address the counterfactual nature of the learning problem through propensity scoring. Next, we prove generalization error bounds that account for the variance of the propensity-weighted empirical risk estimator. These constructive bounds give rise to the Counterfactual Risk Minimization {(CRM)} principle. We show how {CRM} can be used to derive a new learning method -- called Policy Optimizer for Exponential Models {(POEM)} -- for learning stochastic linear rules for structured output prediction. We present a decomposition of the {POEM} objective that enables efficient stochastic gradient optimization. {POEM} is evaluated on several multi-label classification problems showing substantially improved robustness and generalization performance compared to the state-of-the-art.}
}
@book{Zhang_Advances_2016,
  year={2016},
  abstract={Predicting user responses, such as click-through rate and conversion rate, are critical in many web applications including web search, personalised recommendation, and online advertising. Different from continuous raw features that we usually found in the image and audio domains, the input features in web space are always of multi-field and are mostly discrete and categorical while their dependencies are little known. Major user response prediction models have to either limit themselves to linear models or require manually building up high-order combination features. The former loses the ability of exploring feature interactions, while the latter results in a heavy computation in the large feature space. To tackle the issue, we propose two novel models using deep neural networks {(DNNs)} to automatically learn effective patterns from categorical feature interactions and make predictions of users’ ad clicks. To get our {DNNs} efficiently work, we propose to leverage three feature transformation methods, i.e., factorisation machines {(FMs),} restricted Boltzmann machines {(RBMs)} and denoising auto-encoders {(DAEs).} This paper presents the structure of our models and their efficient training algorithms. The large-scale experiments with real-world data demonstrate that our methods work better than major state-of-the-art models.},
  volume={9626},
  doi={10.1007/978-3-319-30671-1_4},
  title={Deep Learning over Multi-field Categorical Data},
  isbn={9783319306704},
  author={Zhang, Weinan and Du, Tianming and Wang, Jun}
}
@article{Guo_Entity_2016,
  year={2016},
  author={Guo, Cheng and Berkhahn, Felix},
  title={Entity Embeddings of Categorical Variables},
  abstract={We map categorical variables in a function approximation problem into Euclidean spaces, which are the entity embeddings of the categorical variables. The mapping is learned by a neural network during the standard supervised training process. Entity embedding not only reduces memory usage and speeds up neural networks compared with one-hot encoding, but more importantly by mapping similar values close to each other in the embedding space it reveals the intrinsic properties of the categorical variables. We applied it successfully in a recent Kaggle competition and were able to reach the third position with relative simple features. We further demonstrate in this paper that entity embedding helps the neural network to generalize better when the data is sparse and statistics is unknown. Thus it is especially useful for datasets with lots of high cardinality features, where other methods tend to overfit. We also demonstrate that the embeddings obtained from the trained neural network boost the performance of all tested machine learning methods considerably when used as the input features instead. As entity embedding defines a distance measure for categorical variables it can be used for visualizing categorical data and for data clustering.},
  file={/Users/pchalasani/Documents/ReadCube Media/Guo et al-2016.pdf}
}
@book{Zhai_DeepIntent_2016,
  year={2016},
  abstract={In this paper, we investigate the use of recurrent neural networks {(RNNs)} in the context of search-based online advertising. We use {RNNs} to map both queries and ads to real valued vectors, with which the relevance of a given (query, ad) pair can be easily computed. On top of the {RNN,} we propose a novel attention network, which learns to assign attention scores to different word locations according to their intent importance (hence the name {DeepIntent).} The vector output of a sequence is thus computed by a weighted sum of the hidden states of the {RNN} at each word according their attention scores. We perform end-to-end training of both the {RNN} and attention network under the guidance of user click logs, which are sampled from a commercial search engine. We show that in most cases the attention network improves the quality of learned vector representations, evaluated by {AUC} on a manually labeled dataset. Moreover, we highlight the effectiveness of the learned attention scores from two aspects: query rewriting and a modified {BM25} metric. We show that using the learned attention scores, one is able to produce sub-queries that are of better qualities than those of the state-of-the-art methods. Also, by modifying the term frequency with the attention scores in a standard {BM25} formula, one is able to improve its performance evaluated by {AUC.}},
  doi={10.1145/2939672.2939759},
  publisher={acm},
  isbn={9781450342322},
  author={Zhai, Shuangfei and Chang, Keng-hao and Zhang, Ruofei and Zhang, Zhongfei},
  file={/Users/pchalasani/Documents/ReadCube Media/Zhai et al-2016-acm.pdf;/Users/pchalasani/Documents/ReadCube Media/Zhai et al-2016-acm 1.pdf}
}
@article{Rasmus_Semi_2015,
  year={2015},
  author={Rasmus, Antti and Valpola, Harri and Honkala, Mikko and Berglund, Mathias and Raiko, Tapani},
  title={{Semi-Supervised} Learning with Ladder Networks},
  abstract={We combine supervised learning with unsupervised learning in deep neural networks. The proposed model is trained to simultaneously minimize the sum of supervised and unsupervised cost functions by backpropagation, avoiding the need for layer-wise pre-training. Our work builds on the Ladder network proposed by Valpola (2015), which we extend by combining the model with supervision. We show that the resulting model reaches state-of-the-art performance in semi-supervised {MNIST} and {CIFAR-10} classification, in addition to permutation-invariant {MNIST} classification with all labels.}
}
@article{Pezeshki_Deconstructing_2015,
  year={2015},
  author={Pezeshki, Mohammad and Fan, Linxi and Brakel, Philemon and Courville, Aaron and Bengio, Yoshua},
  title={Deconstructing the Ladder Network Architecture},
  abstract={The Manual labeling of data is and will remain a costly endeavor. For this reason, semi-supervised learning remains a topic of practical importance. The recently proposed Ladder Network is one such approach that has proven to be very successful. In addition to the supervised objective, the Ladder Network also adds an unsupervised objective corresponding to the reconstruction costs of a stack of denoising autoencoders. Although the empirical results are impressive, the Ladder Network has many components intertwined, whose contributions are not obvious in such a complex architecture. In order to help elucidate and disentangle the different ingredients in the Ladder Network recipe, this paper presents an extensive experimental investigation of variants of the Ladder Network in which we replace or remove individual components to gain more insight into their relative importance. We find that all of the components are necessary for achieving optimal performance, but they do not contribute equally. For semi-supervised tasks, we conclude that the most important contribution is made by the lateral connection, followed by the application of noise, and finally the choice of what we refer to as the ‘combinator function' in the decoder path. We also find that as the number of labeled training examples increases, the lateral connections and reconstruction criterion become less important, with most of the improvement in generalization being due to the injection of noise in each layer. Furthermore, we present a new type of combinator function that outperforms the original design in both fully- and semi-supervised tasks, reducing record test error rates on {Permutation-Invariant} {MNIST} to 0.57\% for the supervised setting, and to 0.97\% and 1.0\% for semi-supervised settings with 1000 and 100 labeled examples respectively.}
}
@article{Borisov_A_2016,
  year={2016},
  author={Borisov, Alexey and Markov, Ilya and de Rijke, Maarten and Serdyukov, Pavel},
  doi={10.1145/2872427.2883033},
  title={A Neural Click Model for Web Search},
  pages={531-541}
}
@book{Wu_Recurrent_2017,
  year={2017},
  abstract={Recommender systems traditionally assume that user profiles and movie attributes are static. Temporal dynamics are purely reactive, that is, they are inferred after they are observed, e.g. after a user's taste has changed or based on hand-engineered temporal bias corrections for movies. We propose Recurrent Recommender Networks {(RRN)} that are able to predict future behavioral trajectories. This is achieved by endowing both users and movies with a Long {Short-Term} Memory {(LSTM)} autoregressive model that captures dynamics, in addition to a more traditional low-rank factorization. On multiple real-world datasets, our model offers excellent prediction accuracy and it is very compact, since we need not learn latent state but rather just the state transition function.},
  doi={10.1145/3018661.3018689},
  publisher={acm},
  isbn={9781450346757},
  author={Wu, {Chao-Yuan} and Ahmed, Amr and Beutel, Alex and Smola, Alexander J and Jing, How}
}
@article{Vieira_Predicting_2015,
  year={2015},
  author={Vieira, Armando},
  title={Predicting online user behaviour using deep learning algorithms},
  abstract={We propose a robust classifier to predict buying intentions based on user behaviour within a large e-commerce website. In this work we compare traditional machine learning techniques with the most advanced deep learning approaches. We show that both Deep Belief Networks and Stacked Denoising {auto-Encoders} achieved a substantial improvement by extracting features from high dimensional data during the pre-train phase. They prove also to be more convenient to deal with severe class imbalance.}
}
@book{Wu_Neural_2015,
  year={2015},
  abstract={In our study, we investigate the effectiveness of different models to the purchasing behaviour at {YOOCHOOSE} website. This paper provide a direct method in modeling the buying pattern in a clicking session by simply using the time-stamp of the clicks and show that the result is comparable to using more massive feature engineering that requires session summarizing. Our proposed method requires much lesser feature engineering and more natural modeling of the click events directly in a typical purchasing session in e-commerce.},
  doi={10.1145/2813448.2813521},
  title={Neural Modeling of Buying Behaviour for {E-Commerce} from Clicking Patterns},
  publisher={acm},
  isbn={9781450336659},
  author={Wu, Zhenzhou and Tan, Bao and Duan, Rubing and Liu, Yong and Goh, Rick}
}
@article{Lipton_A_2015,
  year={2015},
  author={Lipton, Zachary C and Berkowitz, John and Elkan, Charles},
  title={A Critical Review of Recurrent Neural Networks for Sequence Learning},
  abstract={Countless learning tasks require dealing with sequential data. Image captioning, speech synthesis, and music generation all require that a model produce outputs that are sequences. In other domains, such as time series prediction, video analysis, and musical information retrieval, a model must learn from inputs that are sequences. Interactive tasks, such as translating natural language, engaging in dialogue, and controlling a robot, often demand both capabilities. Recurrent neural networks {(RNNs)} are connectionist models that capture the dynamics of sequences via cycles in the network of nodes. Unlike standard feedforward neural networks, recurrent networks retain a state that can represent information from an arbitrarily long context window. Although recurrent neural networks have traditionally been difficult to train, and often contain millions of parameters, recent advances in network architectures, optimization techniques, and parallel computation have enabled successful large-scale learning with them. In recent years, systems based on long short-term memory {(LSTM)} and bidirectional {(BRNN)} architectures have demonstrated ground-breaking performance on tasks as varied as image captioning, language translation, and handwriting recognition. In this survey, we review and synthesize the research that over the past three decades first yielded and then made practical these powerful learning models. When appropriate, we reconcile conflicting notation and nomenclature. Our goal is to provide a self-contained explication of the state of the art together with a historical perspective and references to primary research.}
}
@article{Liu_Multi_2016,
  year={2016},
  author={Liu, Qiang and Wu, Shu and Wang, Liang},
  title={Multi-behavioral Sequential Prediction with Recurrent Log-bilinear Model},
  abstract={With the rapid growth of Internet applications, sequential prediction in collaborative filtering has become an emerging and crucial task. Given the behavioral history of a specific user, predicting his or her next choice plays a key role in improving various online services. Meanwhile, there are more and more scenarios with multiple types of behaviors, while existing works mainly study sequences with a single type of behavior. As a widely used approach, Markov chain based models are based on a strong independence assumption. As two classical neural network methods for modeling sequences, recurrent neural networks cannot well model short-term contexts, and the log-bilinear model is not suitable for long-term contexts. In this paper, we propose a Recurrent {Log-BiLinear} {(RLBL)} model. It can model multiple types of behaviors in historical sequences with behavior-specific transition matrices. {RLBL} applies a recurrent structure for modeling long-term contexts. It models several items in each hidden layer and employs position-specific transition matrices for modeling short-term contexts. Moreover, considering continuous time difference in behavioral history is a key factor for dynamic prediction, we further extend {RLBL} and replace position-specific transition matrices with time-specific transition matrices, and accordingly propose a {Time-Aware} Recurrent {Log-BiLinear} {(TA-RLBL)} model. Experimental results show that the proposed {RLBL} model and {TA-RLBL} model yield significant improvements over the competitive compared methods on three datasets, i.e., {Movielens-1M} dataset, Global Terrorism Database and Tmall dataset with different numbers of behavior types.}
}
@book{Lo_Understanding_2016,
  year={2016},
  abstract={Online e-commerce applications are becoming a primary vehicle for people to find, compare, and ultimately purchase products. One of the fundamental questions that arises in e-commerce is to characterize, understand, and model user long-term purchasing intent, which is important as it allows for personalized and context relevant e-commerce services. In this paper we study user activity and purchasing behavior with the goal of building models of time-varying user purchasing intent. We analyze the purchasing behavior of nearly three million Pinterest users to determine short-term and long-term signals in user behavior that indicate higher purchase intent. We find that users with long-term purchasing intent tend to save and clickthrough on more content. However, as users approach the time of purchase their activity becomes more topically focused and actions shift from saves to searches. We further find that purchase signals in online behavior can exist weeks before a purchase is made and can also be traced across different purchase categories. Finally, we synthesize these insights in predictive models of user purchasing intent. Taken together, our work identifies a set of general principles and signals that can be used to model user purchasing intent across many content discovery applications.},
  doi={10.1145/2939672.2939729},
  isbn={9781450342322},
  author={Lo, Caroline and Frankowski, Dan and Leskovec, Jure}
}
@book{Dalessandro_Scalable_2014,
  year={2014},
  abstract={Internet display advertising is a critical revenue source for publishers and online content providers, and is supported by massive amounts of user and publisher data. Targeting display ads can be improved substantially with machine learning methods, but building many models on massive data becomes prohibitively expensive computationally. This paper presents a combination of strategies, deployed by the online advertising firm Dstillery, for learning many models from extremely high-dimensional data efficiently and without human intervention. This combination includes: {(i)\{\textasciitilde\}A} method for simple-yet-effective transfer learning where a model learned from data that is relatively abundant and cheap is taken as a prior for Bayesian logistic regression trained with stochastic gradient descent {(SGD)} from the more expensive target data. {(ii)\{\textasciitilde\}A} new update rule for automatic learning rate adaptation, to support learning from sparse, high-dimensional data, as well as the integration with adaptive regularization. We present an experimental analysis across 100 different ad campaigns, showing that the transfer learning indeed improves performance across a large number of them, especially at the start of the campaigns. The combined "hands-free" method needs no fiddling with the {SGD} learning rate, and we show that it is just as effective as using expensive grid search to set the regularization parameter for each campaign.},
  doi={10.1145/2623330.2623349},
  isbn={9781450329569},
  author={Dalessandro, Brian and Chen, Daizhuo and Raeder, Troy and Perlich, Claudia and Williams, Melinda and Provost, Foster}
}
@article{Archak_Mining_2010,
  year={2010},
  author={Archak, Nikolay and Mirrokni, Vahab S and Muthukrishnan, S},
  doi={10.1145/1772690.1772695},
  title={Mining advertiser-specific user behavior using adfactors},
  pages={31}
}
@book{Vosoughi_Tweet2Vec_2016,
  year={2016},
  abstract={We present {Tweet2Vec,} a novel method for generating general-purpose vector representation of tweets. The model learns tweet embeddings using character-level {CNN-LSTM} encoder-decoder. We trained our model on 3 million, randomly selected English-language tweets. The model was evaluated using two methods: tweet semantic similarity and tweet sentiment categorization, outperforming the previous state-of-the-art in both tasks. The evaluations demonstrate the power of the tweet embeddings generated by our model for various tweet categorization tasks. The vector representations generated by our model are generic, and hence can be applied to a variety of tasks. Though the model presented in this paper is trained on English-language tweets, the method presented can be used to learn tweet embeddings for different languages.},
  doi={10.1145/2911451.2914762},
  isbn={9781450340694},
  author={Vosoughi, Soroush and Vijayaraghavan, Prashanth and Roy, Deb}
}
@article{Devooght_Collaborative_2016,
  year={2016},
  author={Devooght, Robin and Bersini, Hugues},
  title={Collaborative Filtering with Recurrent Neural Networks},
  abstract={We show that collaborative filtering can be viewed as a sequence prediction problem, and that given this interpretation, recurrent neural networks offer very competitive approach. In particular we study how the long short-term memory {(LSTM)} can be applied to collaborative filtering, and how it compares to standard nearest neighbors and matrix factorization methods on movie recommendation. We show that the {LSTM} is competitive in all aspects, and largely outperforms other methods in terms of item coverage and short term predictions.}
}
@book{Wu_Using_2016,
  year={2016},
  abstract={Implicit feedback is a key source of information for many recommendation and personalization approaches. However, using it typically requires multiple episodes of interaction and roundtrips to a recommendation engine. This adds latency and neglects the opportunity of immediate personalization for a user while the user is navigating recommendations. We propose a novel strategy to address the above problem in a principled manner. The key insight is that as we observe a user's interactions, it reveals much more information about her desires. We exploit this by inferring the within-session user intent on-the-fly based on navigation interactions, since they offer valuable clues into a user's current state of mind. Using navigation patterns and adapting recommendations in real-time creates an opportunity to provide more accurate recommendations. By prefetching a larger amount of content, this can be carried out entirely in the client (such as a browser) without added latency. We define a new Bayesian model with an efficient inference algorithm. We demonstrate significant improvements with this novel approach on a real-world, large-scale dataset from Netflix on the problem of adapting the recommendations on a user's homepage.},
  doi={10.1145/2959100.2959174},
  isbn={9781450340359},
  author={Wu, {Chao-Yuan} and Alvino, Christopher V and Smola, Alexander J and Basilico, Justin}
}
@book{Ikonomovska_Real_2015,
  year={2015},
  abstract={We study online meta-learners for real-time bid prediction that predict by selecting a single best predictor among several subordinate prediction algorithms, here called "experts". These predictors belong to the family of context-dependent past performance estimators that make a prediction only when the instance to be predicted falls within their areas of expertise. Within the advertising ecosystem, it is very common for the contextual information to be incomplete, hence, it is natural for some of the experts to abstain from making predictions on some of the instances. Experts' areas of expertise can overlap, which makes their predictions less suitable for merging; as such, they lend themselves better to the problem of best expert selection. In addition, their performance varies over time, which gives the expert selection problem a non-stochastic, adversarial flavor. In this paper we propose to use probability sampling (via Thompson Sampling) as a meta-learning algorithm that samples from the pool of experts for the purpose of bid prediction. We show performance results from the comparison of our approach to multiple state-of-the-art algorithms using exploration scavenging on a log file of over 300 million ad impressions, as well as comparison to a baseline rule-based model using production traffic from a leading {DSP} platform.},
  doi={10.1145/2783258.2788586},
  isbn={9781450336642},
  author={Ikonomovska, Elena and Jafarpour, Sina and Dasdan, Ali}
}
@article{Graves_Adaptive_2016,
  year={2016},
  author={Graves, Alex},
  title={Adaptive Computation Time for Recurrent Neural Networks},
  abstract={This paper introduces Adaptive Computation Time {(ACT),} an algorithm that allows recurrent neural networks to learn how many computational steps to take between receiving an input and emitting an output. {ACT} requires minimal changes to the network architecture, is deterministic and differentiable, and does not add any noise to the parameter gradients. Experimental results are provided for four synthetic problems: determining the parity of binary vectors, applying binary logic operations, adding integers, and sorting real numbers. Overall, performance is dramatically improved by the use of {ACT,} which successfully adapts the number of computational steps to the requirements of the problem. We also present character-level language modelling results on the Hutter prize Wikipedia dataset. In this case {ACT} does not yield large gains in performance; however it does provide intriguing insight into the structure of the data, with more computation allocated to harder-to-predict transitions, such as spaces between words and ends of sentences. This suggests that {ACT} or other adaptive computation methods could provide a generic method for inferring segment boundaries in sequence data.}
}
@article{Hinton_Distilling_2015,
  year={2015},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  title={Distilling the Knowledge in a Neural Network},
  abstract={A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on {MNIST} and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.}
}
@article{Joulin_Inferring_2015,
  year={2015},
  author={Joulin, Armand and Mikolov, Tomas},
  title={Inferring Algorithmic Patterns with {Stack-Augmented} Recurrent Nets},
  abstract={Despite the recent achievements in machine learning, we are still very far from achieving real artificial intelligence. In this paper, we discuss the limitations of standard deep learning approaches and show that some of these limitations can be overcome by learning how to grow the complexity of a model in a structured way. Specifically, we study the simplest sequence prediction problems that are beyond the scope of what is learnable with standard recurrent networks, algorithmically generated sequences which can only be learned by models which have the capacity to count and to memorize sequences. We show that some basic algorithms can be learned from sequential data using a recurrent network associated with a trainable memory.}
}
@book{Oentaryo_Predicting_2014,
  year={2014},
  abstract={Mobile advertising has recently seen dramatic growth, fueled by the global proliferation of mobile phones and devices. The task of predicting ad response is thus crucial for maximizing business revenue. However, ad response data change dynamically over time, and are subject to cold-start situations in which limited history hinders reliable prediction. There is also a need for a robust regression estimation for high prediction accuracy, and good ranking to distinguish the impacts of different ads. To this end, we develop a Hierarchical Importance-aware Factorization Machine {(HIFM),} which provides an effective generic latent factor framework that incorporates importance weights and hierarchical learning. Comprehensive empirical studies on a real-world mobile advertising dataset show that {HIFM} outperforms the contemporary temporal latent factor models. The results also demonstrate the efficacy of the {HIFM's} importance-aware and hierarchical learning in improving the overall prediction and prediction in cold-start scenarios, respectively.},
  doi={10.1145/2556195.2556240},
  isbn={9781450323512},
  author={Oentaryo, Richard and Lim, {Ee-Peng} and Low, {Jia-Wei} and Lo, David and Finegold, Michael},
  file={/Users/pchalasani/Dropbox/MediaMath/Papers/ML/oentaryo2014.pdf}
}
@article{Eckles_Design_2014,
  year={2014},
  author={Eckles, Dean and Karrer, Brian and Ugander, Johan},
  title={Design and analysis of experiments in networks: Reducing bias from interference},
  abstract={Estimating the effects of interventions in networks is complicated when the units are interacting, such that the outcomes for one unit may depend on the treatment assignment and behavior of many or all other units (i.e., there is interference). When most or all units are in a single connected component, it is impossible to directly experimentally compare outcomes under two or more global treatment assignments since the network can only be observed under a single assignment. Familiar formalism, experimental designs, and analysis methods assume the absence of these interactions, and result in biased estimators of causal effects of interest. While some assumptions can lead to unbiased estimators, these assumptions are generally unrealistic, and we focus this work on realistic assumptions. Thus, in this work, we evaluate methods for designing and analyzing randomized experiments that aim to reduce this bias and thereby reduce overall error. In design, we consider the ability to perform random assignment to treatments that is correlated in the network, such as through graph cluster randomization. In analysis, we consider incorporating information about the treatment assignment of network neighbors. We prove sufficient conditions for bias reduction through both design and analysis in the presence of potentially global interference. Through simulations of the entire process of experimentation in networks, we measure the performance of these methods under varied network structure and varied social behaviors, finding substantial bias and error reductions. These improvements are largest for networks with more clustering and data generating processes with both stronger direct effects of the treatment and stronger interactions between units.},
  file={/Users/pchalasani/Documents/ReadCube Media/Eckles et al-2014.pdf}
}
@article{Aronow_Estimating_2013,
  year={2013},
  author={Aronow, Peter M and Samii, Cyrus},
  title={Estimating Average Causal Effects Under Interference Between Units},
  abstract={This paper presents a randomization-based framework for estimating causal effects under interference between units. The framework integrates three components: (i) an experimental design that defines the probability distribution of treatment assignments, (ii) a mapping that relates experimental treatment assignments to exposures received by units in the experiment, and (iii) estimands that make use of the experiment to answer questions of substantive interest. Using this framework, we develop the case of estimating average unit-level causal effects from a randomized experiment with interference of arbitrary but known form. The resulting estimators are based on inverse probability weighting. We provide randomization-based variance estimators that account for the complex clustering that can occur when interference is present. We also establish consistency and asymptotic normality under local dependence assumptions. We discuss refinements including covariate-adjusted effect estimators and ratio estimation. We illustrate and assess empirical performance with a naturalistic simulation using network data from American high schools.},
  file={/Users/pchalasani/Documents/ReadCube Media/Aronow et al-2013.pdf}
}
@article{Stitelman_Estimating_2011,
  year={2011},
  journal={Data Mining and  …},
  author={Stitelman, O and Dalessandro, B and Perlich, C},
  title={Estimating the effect of online display advertising on browser conversion},
  abstract={... Chan et al., presented at last year's {KDD,} proposed the use of several related methods that are
able to estimate the effect of advertising in observational data[1]. In particular, their paper focused
on estimating the causal effect of ad- vertising among those shown the ...
},
  file={/Users/pchalasani/Documents/ReadCube Media/Stitelman et al-2011-  for Advertising (ADKDD.pdf}
}
@article{Dalessandro_Causally_2012,
  year={2012},
  journal={Proceedings of the Sixth  …},
  author={Dalessandro, B and Perlich, C and Stitelman, O},
  title={Causally motivated attribution for online advertising},
  abstract={... et al. [22] and Chan et al. [4]. The ... trivial matter. In many cases, it may be downright
impossible. We defined each user at time t as an outcome Y t, a set of covariates
Wt and a possible ad, served by one of K chan- nels. In most ...
}
}
@article{Johnson_Ghost_2015,
  year={2015},
  journal={Available at {SSRN}},
  author={Johnson, {GA} and Lewis, {RA} and Nubbemeyer, {EI}},
  title={Ghost ads: Improving the economics of measuring ad effectiveness},
  abstract={Abstract To measure the effects of advertising, marketers must know how consumers would behave had they not seen the ads. We develop a methodology we call {'Ghost} Ads,'which facilitates this comparison by identifying the control-group counterparts of the exposed },
  file={/Users/pchalasani/Documents/ReadCube Media/Johnson et al-2015-Available at SSRN.pdf}
}
@article{Chickering_A_1996,
  year={1996},
  journal={Proceedings of the National Conference on  …},
  author={Chickering, {DM} and Pearl, J},
  title={A clinician's tool for analyzing non-compliance},
  abstract={Abstract We describe a computer program to assist a clinician with assessing the efficacy of treatments in experimental studies for which treatment assignment is random but subject compliance is imperfect. The major difficulty in such studies is that treatment efficacy is not },
  file={/Users/pchalasani/Documents/ReadCube Media/Chickering et al-1996-Proceedings of the National Conference on.pdf}
}
@article{Resnik_Gibbs_2010,
  year={2010},
  author={Resnik, P and Hardisty, E},
  title={Gibbs sampling for the uninitiated},
  abstract={Abstract: This document is intended for computer scientists who would like to try out a Markov Chain Monte Carlo {(MCMC)} technique, particularly to do inference with Bayesian models on problems related to text processing. We try to keep theory to the absolute }
}
